# HG changeset patch
# User Crypt <snandaku@cisco.com>
# Date 1340152183 25200
# Node ID 4e21996fc0f04223ef1339138b9fcf319d65f530
# Parent 155f67c2c5789ca51d4edc49403199e9e8434e48
 Bug 691234 Webrtc Backend With Basic Snapshot Support and Cleanup

diff --git a/content/media/Makefile.in b/content/media/Makefile.in
--- a/content/media/Makefile.in
+++ b/content/media/Makefile.in
@@ -16,8 +16,6 @@
 EXPORTS = \
   AudioSegment.h \
   FileBlockCache.h \
-  MediaEngine.h \
-  MediaEngineDefault.h \
   MediaResource.h \
   MediaSegment.h \
   MediaStreamGraph.h \
@@ -39,7 +37,6 @@
 CPPSRCS = \
   AudioSegment.cpp \
   FileBlockCache.cpp \
-  MediaEngineDefault.cpp \
   MediaResource.cpp \
   MediaStreamGraph.cpp \
   nsAudioAvailableEventManager.cpp \
@@ -87,6 +84,7 @@
 PARALLEL_DIRS += plugins
 endif
 
+PARALLEL_DIRS += webrtc
 TEST_DIRS += test
 
 FORCE_STATIC_LIB = 1
diff --git a/content/media/MediaEngine.h b/content/media/MediaEngine.h
deleted file mode 100644
--- a/content/media/MediaEngine.h
+++ /dev/null
@@ -1,113 +0,0 @@
-/* This Source Code Form is subject to the terms of the Mozilla Public
- * License, v. 2.0. If a copy of the MPL was not distributed with this file,
- * You can obtain one at http://mozilla.org/MPL/2.0/. */
-
-#ifndef MEDIAENGINE_H_
-#define MEDIAENGINE_H_
-
-#include "nsIDOMFile.h"
-#include "nsDOMMediaStream.h"
-#include "MediaStreamGraph.h"
-
-namespace mozilla {
-
-/**
- * Abstract interface for managing audio and video devices. Each platform
- * must implement a concrete class that will map these classes and methods
- * to the appropriate backend. For example, on Desktop platforms, these will
- * correspond to equivalent webrtc (GIPS) calls, and on B2G they will map to
- * a Gonk interface.
- */
-class MediaEngineVideoSource;
-class MediaEngineAudioSource;
-
-class MediaEngine
-{
-public:
-  virtual ~MediaEngine() {};
-
-  /* Populate an array of video sources in the nsTArray. Also include devices
-   * that are currently unavailable. */
-  virtual void EnumerateVideoDevices(nsTArray<nsRefPtr<MediaEngineVideoSource> >*) = 0;
-
-  /* Populate an array of audio sources in the nsTArray. Also include devices
-   * that are currently unavailable. */
-  virtual void EnumerateAudioDevices(nsTArray<nsRefPtr<MediaEngineAudioSource> >*) = 0;
-};
-
-/**
- * Common abstract base class for audio and video sources.
- */
-class MediaEngineSource : public nsISupports
-{
-public:
-  virtual ~MediaEngineSource() {};
-
-  /* Populate the human readable name of this device in the nsAString */
-  virtual void GetName(nsAString&) = 0;
-
-  /* Populate the UUID of this device in the nsAString */
-  virtual void GetUUID(nsAString&) = 0;
-
-  /* This call reserves but does not start the device. */
-  virtual already_AddRefed<nsDOMMediaStream> Allocate() = 0;
-
-  /* Release the device back to the system. */
-  virtual nsresult Deallocate() = 0;
-
-  /* Start the device and add the track to the provided SourceMediaStream, with
-   * the provided TrackID. You may start appending data to the track
-   * immediately after. */
-  virtual nsresult Start(SourceMediaStream*, TrackID) = 0;
-
-  /* Take a snapshot from this source. In the case of video this is a single
-   * image, and for audio, it is a snippet lasting aDuration milliseconds. The
-   * duration argument is ignored for a MediaEngineVideoSource.
-   */
-  virtual nsresult Snapshot(PRUint32 aDuration, nsIDOMFile** aFile) = 0;
-
-  /* Stop the device and release the corresponding MediaStream */
-  virtual nsresult Stop() = 0;
-
-  /* It is an error to call Start() before an Allocate(), and Stop() before
-   * a Start(). Only Allocate() may be called after a Deallocate(). */
-};
-
-/**
- * Video source and friends.
- */
-enum MediaEngineVideoCodecType {
-  kVideoCodecH263,
-  kVideoCodecVP8,
-  kVideoCodecI420
-};
-
-struct MediaEngineVideoOptions {
-  PRUint32 mWidth;
-  PRUint32 mHeight;
-  PRUint32 mMaxFPS;
-  MediaEngineVideoCodecType codecType;
-};
-
-class MediaEngineVideoSource : public MediaEngineSource
-{
-public:
-  virtual ~MediaEngineVideoSource() {};
-
-  /* Return a MediaEngineVideoOptions struct with appropriate values for all
-   * fields. */
-  virtual MediaEngineVideoOptions GetOptions() = 0;
-};
-
-/**
- * Audio source and friends.
- */
-class MediaEngineAudioSource : public MediaEngineSource
-{
-public:
-  virtual ~MediaEngineAudioSource() {};
-};
-
-}
-
-#endif /* MEDIAENGINE_H_ */
diff --git a/content/media/MediaEngineDefault.cpp b/content/media/MediaEngineDefault.cpp
deleted file mode 100644
--- a/content/media/MediaEngineDefault.cpp
+++ /dev/null
@@ -1,314 +0,0 @@
-/* This Source Code Form is subject to the terms of the Mozilla Public
- * License, v. 2.0. If a copy of the MPL was not distributed with this file,
- * You can obtain one at http://mozilla.org/MPL/2.0/. */
-
-#include "MediaEngineDefault.h"
-
-#include "nsCOMPtr.h"
-#include "nsDOMFile.h"
-#include "nsILocalFile.h"
-
-#ifdef MOZ_WIDGET_ANDROID
-#include "AndroidBridge.h"
-#include "nsISupportsUtils.h"
-#endif
-
-#define WIDTH 320
-#define HEIGHT 240
-#define FPS 10
-#define CHANNELS 1
-#define RATE USECS_PER_S
-
-namespace mozilla {
-
-NS_IMPL_THREADSAFE_ISUPPORTS1(MediaEngineDefaultVideoSource, nsITimerCallback)
-/**
- * Default video source.
- */
-void
-MediaEngineDefaultVideoSource::GetName(nsAString& aName)
-{
-  aName.Assign(NS_LITERAL_STRING("Default Video Device"));
-  return;
-}
-
-void
-MediaEngineDefaultVideoSource::GetUUID(nsAString& aUUID)
-{
-  aUUID.Assign(NS_LITERAL_STRING("1041FCBD-3F12-4F7B-9E9B-1EC556DD5676"));
-  return;
-}
-
-already_AddRefed<nsDOMMediaStream>
-MediaEngineDefaultVideoSource::Allocate()
-{
-  if (mState != kReleased) {
-    return NULL;
-  }
-
-  mState = kAllocated;
-  return nsDOMMediaStream::CreateInputStream();
-}
-
-nsresult
-MediaEngineDefaultVideoSource::Deallocate()
-{
-  if (mState != kStopped && mState != kAllocated) {
-    return NS_ERROR_FAILURE;
-  }
-  mState = kReleased;
-  return NS_OK;
-}
-
-MediaEngineVideoOptions
-MediaEngineDefaultVideoSource::GetOptions()
-{
-  MediaEngineVideoOptions aOpts;
-  aOpts.mWidth = WIDTH;
-  aOpts.mHeight = HEIGHT;
-  aOpts.mMaxFPS = FPS;
-  aOpts.codecType = kVideoCodecI420;
-  return aOpts;
-}
-
-nsresult
-MediaEngineDefaultVideoSource::Start(SourceMediaStream* aStream, TrackID aID)
-{
-  if (mState != kAllocated) {
-    return NS_ERROR_FAILURE;
-  }
-
-  mTimer = do_CreateInstance(NS_TIMER_CONTRACTID);
-  if (!mTimer) {
-    return NS_ERROR_FAILURE;
-  }
-
-  mSource = aStream;
-
-  // Allocate a single blank Image
-  layers::Image::Format format = layers::Image::PLANAR_YCBCR;
-  mImageContainer = layers::LayerManager::CreateImageContainer();
-
-  nsRefPtr<layers::Image> image = mImageContainer->CreateImage(&format, 1);
-
-  int len = ((WIDTH * HEIGHT) * 3 / 2);
-  mImage = static_cast<layers::PlanarYCbCrImage*>(image.get());
-  PRUint8* frame = (PRUint8*) PR_Malloc(len);
-  memset(frame, 0x80, len); // Gray
-
-  const PRUint8 lumaBpp = 8;
-  const PRUint8 chromaBpp = 4;
-
-  layers::PlanarYCbCrImage::Data data;
-  data.mYChannel = frame;
-  data.mYSize = gfxIntSize(WIDTH, HEIGHT);
-  data.mYStride = WIDTH * lumaBpp / 8.0;
-  data.mCbCrStride = WIDTH * chromaBpp / 8.0;
-  data.mCbChannel = frame + HEIGHT * data.mYStride;
-  data.mCrChannel = data.mCbChannel + HEIGHT * data.mCbCrStride / 2;
-  data.mCbCrSize = gfxIntSize(WIDTH / 2, HEIGHT / 2);
-  data.mPicX = 0;
-  data.mPicY = 0;
-  data.mPicSize = gfxIntSize(WIDTH, HEIGHT);
-  data.mStereoMode = layers::STEREO_MODE_MONO;
-
-  // SetData copies data, so we can free the frame
-  mImage->SetData(data);
-  PR_Free(frame);
-
-
-  // AddTrack takes ownership of segment
-  VideoSegment *segment = new VideoSegment();
-  segment->AppendFrame(image.forget(), USECS_PER_S / FPS, gfxIntSize(WIDTH, HEIGHT));
-  mSource->AddTrack(aID, RATE, 0, segment);
-
-  // We aren't going to add any more tracks
-  mSource->AdvanceKnownTracksTime(STREAM_TIME_MAX);
-
-  // Remember TrackID so we can end it later
-  mTrackID = aID;
-
-  // Start timer for subsequent frames
-  mTimer->InitWithCallback(this, 1000 / FPS, nsITimer::TYPE_REPEATING_SLACK);
-  mState = kStarted;
-
-  return NS_OK;
-}
-
-nsresult
-MediaEngineDefaultVideoSource::Stop()
-{
-  if (mState != kStarted) {
-    return NS_ERROR_FAILURE;
-  }
-  if (!mTimer) {
-    return NS_ERROR_FAILURE;
-  }
-
-  mTimer->Cancel();
-  mTimer = NULL;
-
-  mSource->EndTrack(mTrackID);
-  mSource->Finish();
-
-  mState = kStopped;
-  return NS_OK;
-}
-
-nsresult
-MediaEngineDefaultVideoSource::Snapshot(PRUint32 aDuration, nsIDOMFile** aFile)
-{
-  *aFile = nsnull;
-
-#ifndef MOZ_WIDGET_ANDROID
-  return NS_ERROR_NOT_IMPLEMENTED;
-#else
-  if (!AndroidBridge::Bridge()) {
-    return NS_ERROR_UNEXPECTED;
-  }
-
-  nsAutoString filePath;
-  AndroidBridge::Bridge()->ShowFilePickerForMimeType(filePath, NS_LITERAL_STRING("image/*"));
-
-  nsCOMPtr<nsIFile> file;
-  nsresult rv = NS_NewLocalFile(filePath, false, getter_AddRefs(file));
-  NS_ENSURE_SUCCESS(rv, rv);
-
-  NS_ADDREF(*aFile = new nsDOMFileFile(file));
-  return NS_OK;
-#endif
-}
-
-NS_IMETHODIMP
-MediaEngineDefaultVideoSource::Notify(nsITimer* aTimer)
-{
-  VideoSegment segment;
-
-  nsRefPtr<layers::PlanarYCbCrImage> image = mImage;
-  segment.AppendFrame(image.forget(), USECS_PER_S / FPS, gfxIntSize(WIDTH, HEIGHT));
-  mSource->AppendToTrack(mTrackID, &segment);
-
-  return NS_OK;
-}
-
-NS_IMPL_THREADSAFE_ISUPPORTS1(MediaEngineDefaultAudioSource, nsITimerCallback)
-/**
- * Default audio source.
- */
-void
-MediaEngineDefaultAudioSource::GetName(nsAString& aName)
-{
-  aName.Assign(NS_LITERAL_STRING("Default Audio Device"));
-  return;
-}
-
-void
-MediaEngineDefaultAudioSource::GetUUID(nsAString& aUUID)
-{
-  aUUID.Assign(NS_LITERAL_STRING("B7CBD7C1-53EF-42F9-8353-73F61C70C092"));
-  return;
-}
-
-already_AddRefed<nsDOMMediaStream>
-MediaEngineDefaultAudioSource::Allocate()
-{
-  if (mState != kReleased) {
-    return NULL;
-  }
-  mState = kAllocated;
-  return nsDOMMediaStream::CreateInputStream();
-}
-
-nsresult
-MediaEngineDefaultAudioSource::Deallocate()
-{
-  if (mState != kStopped && mState != kAllocated) {
-    return NS_ERROR_FAILURE;
-  }
-  mState = kReleased;
-  return NS_OK;
-}
-
-nsresult
-MediaEngineDefaultAudioSource::Start(SourceMediaStream* aStream, TrackID aID)
-{
-  if (mState != kAllocated) {
-    return NULL;
-  }
-
-  mTimer = do_CreateInstance(NS_TIMER_CONTRACTID);
-  if (!mTimer) {
-    return NULL;
-  }
-
-  mSource = aStream;
-
-  // AddTrack will take ownership of segment
-  AudioSegment* segment = new AudioSegment();
-  segment->Init(CHANNELS);
-  mSource->AddTrack(aID, RATE, 0, segment);
-
-  // We aren't going to add any more tracks
-  mSource->AdvanceKnownTracksTime(STREAM_TIME_MAX);
-
-  // Remember TrackID so we can finish later
-  mTrackID = aID;
-
-  // 1 Audio frame per Video frame
-  mTimer->InitWithCallback(this, 1000 / FPS, nsITimer::TYPE_REPEATING_SLACK);
-  mState = kStarted;
-
-  return NS_OK;
-}
-
-nsresult
-MediaEngineDefaultAudioSource::Stop()
-{
-  if (mState != kStarted) {
-    return NS_ERROR_FAILURE;
-  }
-  if (!mTimer) {
-    return NS_ERROR_FAILURE;
-  }
-
-  mTimer->Cancel();
-  mTimer = NULL;
-
-  mSource->EndTrack(mTrackID);
-  mSource->Finish();
-
-  mState = kStopped;
-  return NS_OK;
-}
-
-nsresult
-MediaEngineDefaultAudioSource::Snapshot(PRUint32 aDuration, nsIDOMFile** aFile)
-{
-   return NS_ERROR_NOT_IMPLEMENTED;
-}
-
-NS_IMETHODIMP
-MediaEngineDefaultAudioSource::Notify(nsITimer* aTimer)
-{
-  AudioSegment segment;
-  segment.Init(CHANNELS);
-  segment.InsertNullDataAtStart(1);
-
-  mSource->AppendToTrack(mTrackID, &segment);
-
-  return NS_OK;
-}
-
-void
-MediaEngineDefault::EnumerateVideoDevices(nsTArray<nsRefPtr<MediaEngineVideoSource> >* aVSources) {
-  aVSources->AppendElement(mVSource);
-  return;
-}
-
-void
-MediaEngineDefault::EnumerateAudioDevices(nsTArray<nsRefPtr<MediaEngineAudioSource> >* aASources) {
-  aASources->AppendElement(mASource);
-  return;
-}
-
-} // namespace mozilla
diff --git a/content/media/MediaEngineDefault.h b/content/media/MediaEngineDefault.h
deleted file mode 100644
--- a/content/media/MediaEngineDefault.h
+++ /dev/null
@@ -1,115 +0,0 @@
-/* This Source Code Form is subject to the terms of the Mozilla Public
- * License, v. 2.0. If a copy of the MPL was not distributed with this file,
- * You can obtain one at http://mozilla.org/MPL/2.0/. */
-
-#ifndef MEDIAENGINEDEFAULT_H_
-#define MEDIAENGINEDEFAULT_H_
-
-#include "prmem.h"
-#include "nsITimer.h"
-
-#include "nsCOMPtr.h"
-#include "nsDOMMediaStream.h"
-#include "nsComponentManagerUtils.h"
-
-#include "Layers.h"
-#include "VideoUtils.h"
-#include "MediaEngine.h"
-#include "ImageLayers.h"
-#include "VideoSegment.h"
-#include "AudioSegment.h"
-#include "StreamBuffer.h"
-#include "MediaStreamGraph.h"
-
-namespace mozilla {
-
-/**
- * The default implementation of the MediaEngine interface.
- */
-
-enum DefaultEngineState {
-  kAllocated,
-  kStarted,
-  kStopped,
-  kReleased
-};
-
-class MediaEngineDefaultVideoSource : public nsITimerCallback,
-                                      public MediaEngineVideoSource
-{
-public:
-  MediaEngineDefaultVideoSource() : mTimer(nsnull), mState(kReleased) {}
-  ~MediaEngineDefaultVideoSource(){};
-
-  virtual void GetName(nsAString&);
-  virtual void GetUUID(nsAString&);
-
-  virtual MediaEngineVideoOptions GetOptions();
-  virtual already_AddRefed<nsDOMMediaStream> Allocate();
-
-  virtual nsresult Deallocate();
-  virtual nsresult Start(SourceMediaStream*, TrackID);
-  virtual nsresult Stop();
-  virtual nsresult Snapshot(PRUint32 aDuration, nsIDOMFile** aFile);
-
-  NS_DECL_ISUPPORTS
-  NS_DECL_NSITIMERCALLBACK
-
-protected:
-  TrackID mTrackID;
-  nsCOMPtr<nsITimer> mTimer;
-  nsRefPtr<layers::ImageContainer> mImageContainer;
-
-  DefaultEngineState mState;
-  SourceMediaStream* mSource;
-  layers::PlanarYCbCrImage* mImage;
-};
-
-class MediaEngineDefaultAudioSource : public nsITimerCallback,
-                                      public MediaEngineAudioSource
-{
-public:
-  MediaEngineDefaultAudioSource() : mTimer(nsnull), mState(kReleased) {}
-  ~MediaEngineDefaultAudioSource(){};
-
-  virtual void GetName(nsAString&);
-  virtual void GetUUID(nsAString&);
-
-  virtual already_AddRefed<nsDOMMediaStream> Allocate();
-
-  virtual nsresult Deallocate();
-  virtual nsresult Start(SourceMediaStream*, TrackID);
-  virtual nsresult Stop();
-  virtual nsresult Snapshot(PRUint32 aDuration, nsIDOMFile** aFile);
-
-  NS_DECL_ISUPPORTS
-  NS_DECL_NSITIMERCALLBACK
-
-protected:
-  TrackID mTrackID;
-  nsCOMPtr<nsITimer> mTimer;
-
-  DefaultEngineState mState;
-  SourceMediaStream* mSource;
-};
-
-class MediaEngineDefault : public MediaEngine
-{
-public:
-  MediaEngineDefault() {
-    mVSource = new MediaEngineDefaultVideoSource();
-    mASource = new MediaEngineDefaultAudioSource();
-  }
-  ~MediaEngineDefault() {}
-
-  virtual void EnumerateVideoDevices(nsTArray<nsRefPtr<MediaEngineVideoSource> >*);
-  virtual void EnumerateAudioDevices(nsTArray<nsRefPtr<MediaEngineAudioSource> >*);
-
-private:
-  nsRefPtr<MediaEngineVideoSource> mVSource;
-  nsRefPtr<MediaEngineAudioSource> mASource;
-};
-
-}
-
-#endif /* NSMEDIAENGINEDEFAULT_H_ */
diff --git a/content/media/webrtc/Makefile.in b/content/media/webrtc/Makefile.in
new file mode 100644
--- /dev/null
+++ b/content/media/webrtc/Makefile.in
@@ -0,0 +1,33 @@
+# This Source Code Form is subject to the terms of the Mozilla Public
+# License, v. 2.0. If a copy of the MPL was not distributed with this
+# file, You can obtain one at http://mozilla.org/MPL/2.0/.
+
+DEPTH		= ../../..
+topsrcdir	= @top_srcdir@
+srcdir		= @srcdir@
+VPATH		= @srcdir@
+
+include $(DEPTH)/config/autoconf.mk
+
+MODULE = content
+LIBRARY_NAME = gkconwebrtc_s
+LIBXUL_LIBRARY = 1
+
+EXPORTS	+= \
+	MediaEngine.h \
+  MediaEngineDefault.h \
+  MediaEngineWebRTC.h \
+  $(NULL)
+
+CPPSRCS	= \
+	MediaEngineWebRTC.cpp \
+  MediaEngineDefault.cpp \
+	$(NULL)
+
+FORCE_STATIC_LIB = 1
+
+include $(topsrcdir)/config/rules.mk
+
+LOCAL_INCLUDES += \
+  -I$(topsrcdir)/media/webrtc/trunk/src \
+  $(NULL)
diff --git a/content/media/webrtc/MediaEngine.h b/content/media/webrtc/MediaEngine.h
new file mode 100644
--- /dev/null
+++ b/content/media/webrtc/MediaEngine.h
@@ -0,0 +1,113 @@
+/* This Source Code Form is subject to the terms of the Mozilla Public
+ * License, v. 2.0. If a copy of the MPL was not distributed with this file,
+ * You can obtain one at http://mozilla.org/MPL/2.0/. */
+
+#ifndef MEDIAENGINE_H_
+#define MEDIAENGINE_H_
+
+#include "nsIDOMFile.h"
+#include "nsDOMMediaStream.h"
+#include "MediaStreamGraph.h"
+
+namespace mozilla {
+
+/**
+ * Abstract interface for managing audio and video devices. Each platform
+ * must implement a concrete class that will map these classes and methods
+ * to the appropriate backend. For example, on Desktop platforms, these will
+ * correspond to equivalent webrtc (GIPS) calls, and on B2G they will map to
+ * a Gonk interface.
+ */
+class MediaEngineVideoSource;
+class MediaEngineAudioSource;
+
+class MediaEngine
+{
+public:
+  virtual ~MediaEngine() {};
+
+  /* Populate an array of video sources in the nsTArray. Also include devices
+   * that are currently unavailable. */
+  virtual void EnumerateVideoDevices(nsTArray<nsRefPtr<MediaEngineVideoSource> >*) = 0;
+
+  /* Populate an array of audio sources in the nsTArray. Also include devices
+   * that are currently unavailable. */
+  virtual void EnumerateAudioDevices(nsTArray<nsRefPtr<MediaEngineAudioSource> >*) = 0;
+};
+
+/**
+ * Common abstract base class for audio and video sources.
+ */
+class MediaEngineSource : public nsISupports
+{
+public:
+  virtual ~MediaEngineSource() {};
+
+  /* Populate the human readable name of this device in the nsAString */
+  virtual void GetName(nsAString&) = 0;
+
+  /* Populate the UUID of this device in the nsAString */
+  virtual void GetUUID(nsAString&) = 0;
+
+  /* This call reserves but does not start the device. */
+  virtual already_AddRefed<nsDOMMediaStream> Allocate() = 0;
+
+  /* Release the device back to the system. */
+  virtual nsresult Deallocate() = 0;
+
+  /* Start the device and add the track to the provided SourceMediaStream, with
+   * the provided TrackID. You may start appending data to the track
+   * immediately after. */
+  virtual nsresult Start(SourceMediaStream*, TrackID) = 0;
+
+  /* Take a snapshot from this source. In the case of video this is a single
+   * image, and for audio, it is a snippet lasting aDuration milliseconds. The
+   * duration argument is ignored for a MediaEngineVideoSource.
+   */
+  virtual nsresult Snapshot(PRUint32 aDuration, nsIDOMFile** aFile) = 0;
+
+  /* Stop the device and release the corresponding MediaStream */
+  virtual nsresult Stop() = 0;
+
+  /* It is an error to call Start() before an Allocate(), and Stop() before
+   * a Start(). Only Allocate() may be called after a Deallocate(). */
+};
+
+/**
+ * Video source and friends.
+ */
+enum MediaEngineVideoCodecType {
+  kVideoCodecH263,
+  kVideoCodecVP8,
+  kVideoCodecI420
+};
+
+struct MediaEngineVideoOptions {
+  PRUint32 mWidth;
+  PRUint32 mHeight;
+  PRUint32 mMaxFPS;
+  MediaEngineVideoCodecType codecType;
+};
+
+class MediaEngineVideoSource : public MediaEngineSource
+{
+public:
+  virtual ~MediaEngineVideoSource() {};
+
+  /* Return a MediaEngineVideoOptions struct with appropriate values for all
+   * fields. */
+  virtual MediaEngineVideoOptions GetOptions() = 0;
+};
+
+/**
+ * Audio source and friends.
+ */
+class MediaEngineAudioSource : public MediaEngineSource
+{
+public:
+  virtual ~MediaEngineAudioSource() {};
+};
+
+}
+
+#endif /* MEDIAENGINE_H_ */
diff --git a/content/media/webrtc/MediaEngineDefault.cpp b/content/media/webrtc/MediaEngineDefault.cpp
new file mode 100644
--- /dev/null
+++ b/content/media/webrtc/MediaEngineDefault.cpp
@@ -0,0 +1,314 @@
+/* This Source Code Form is subject to the terms of the Mozilla Public
+ * License, v. 2.0. If a copy of the MPL was not distributed with this file,
+ * You can obtain one at http://mozilla.org/MPL/2.0/. */
+
+#include "MediaEngineDefault.h"
+
+#include "nsCOMPtr.h"
+#include "nsDOMFile.h"
+#include "nsILocalFile.h"
+
+#ifdef MOZ_WIDGET_ANDROID
+#include "AndroidBridge.h"
+#include "nsISupportsUtils.h"
+#endif
+
+#define WIDTH 320
+#define HEIGHT 240
+#define FPS 10
+#define CHANNELS 1
+#define RATE USECS_PER_S
+
+namespace mozilla {
+
+NS_IMPL_THREADSAFE_ISUPPORTS1(MediaEngineDefaultVideoSource, nsITimerCallback)
+/**
+ * Default video source.
+ */
+void
+MediaEngineDefaultVideoSource::GetName(nsAString& aName)
+{
+  aName.Assign(NS_LITERAL_STRING("Default Video Device"));
+  return;
+}
+
+void
+MediaEngineDefaultVideoSource::GetUUID(nsAString& aUUID)
+{
+  aUUID.Assign(NS_LITERAL_STRING("1041FCBD-3F12-4F7B-9E9B-1EC556DD5676"));
+  return;
+}
+
+already_AddRefed<nsDOMMediaStream>
+MediaEngineDefaultVideoSource::Allocate()
+{
+  if (mState != kReleased) {
+    return NULL;
+  }
+
+  mState = kAllocated;
+  return nsDOMMediaStream::CreateInputStream();
+}
+
+nsresult
+MediaEngineDefaultVideoSource::Deallocate()
+{
+  if (mState != kStopped && mState != kAllocated) {
+    return NS_ERROR_FAILURE;
+  }
+  mState = kReleased;
+  return NS_OK;
+}
+
+MediaEngineVideoOptions
+MediaEngineDefaultVideoSource::GetOptions()
+{
+  MediaEngineVideoOptions aOpts;
+  aOpts.mWidth = WIDTH;
+  aOpts.mHeight = HEIGHT;
+  aOpts.mMaxFPS = FPS;
+  aOpts.codecType = kVideoCodecI420;
+  return aOpts;
+}
+
+nsresult
+MediaEngineDefaultVideoSource::Start(SourceMediaStream* aStream, TrackID aID)
+{
+  if (mState != kAllocated) {
+    return NS_ERROR_FAILURE;
+  }
+
+  mTimer = do_CreateInstance(NS_TIMER_CONTRACTID);
+  if (!mTimer) {
+    return NS_ERROR_FAILURE;
+  }
+
+  mSource = aStream;
+
+  // Allocate a single blank Image
+  layers::Image::Format format = layers::Image::PLANAR_YCBCR;
+  mImageContainer = layers::LayerManager::CreateImageContainer();
+
+  nsRefPtr<layers::Image> image = mImageContainer->CreateImage(&format, 1);
+
+  int len = ((WIDTH * HEIGHT) * 3 / 2);
+  mImage = static_cast<layers::PlanarYCbCrImage*>(image.get());
+  PRUint8* frame = (PRUint8*) PR_Malloc(len);
+  memset(frame, 0x80, len); // Gray
+
+  const PRUint8 lumaBpp = 8;
+  const PRUint8 chromaBpp = 4;
+
+  layers::PlanarYCbCrImage::Data data;
+  data.mYChannel = frame;
+  data.mYSize = gfxIntSize(WIDTH, HEIGHT);
+  data.mYStride = WIDTH * lumaBpp / 8.0;
+  data.mCbCrStride = WIDTH * chromaBpp / 8.0;
+  data.mCbChannel = frame + HEIGHT * data.mYStride;
+  data.mCrChannel = data.mCbChannel + HEIGHT * data.mCbCrStride / 2;
+  data.mCbCrSize = gfxIntSize(WIDTH / 2, HEIGHT / 2);
+  data.mPicX = 0;
+  data.mPicY = 0;
+  data.mPicSize = gfxIntSize(WIDTH, HEIGHT);
+  data.mStereoMode = layers::STEREO_MODE_MONO;
+
+  // SetData copies data, so we can free the frame
+  mImage->SetData(data);
+  PR_Free(frame);
+
+
+  // AddTrack takes ownership of segment
+  VideoSegment *segment = new VideoSegment();
+  segment->AppendFrame(image.forget(), USECS_PER_S / FPS, gfxIntSize(WIDTH, HEIGHT));
+  mSource->AddTrack(aID, RATE, 0, segment);
+
+  // We aren't going to add any more tracks
+  mSource->AdvanceKnownTracksTime(STREAM_TIME_MAX);
+
+  // Remember TrackID so we can end it later
+  mTrackID = aID;
+
+  // Start timer for subsequent frames
+  mTimer->InitWithCallback(this, 1000 / FPS, nsITimer::TYPE_REPEATING_SLACK);
+  mState = kStarted;
+
+  return NS_OK;
+}
+
+nsresult
+MediaEngineDefaultVideoSource::Stop()
+{
+  if (mState != kStarted) {
+    return NS_ERROR_FAILURE;
+  }
+  if (!mTimer) {
+    return NS_ERROR_FAILURE;
+  }
+
+  mTimer->Cancel();
+  mTimer = NULL;
+
+  mSource->EndTrack(mTrackID);
+  mSource->Finish();
+
+  mState = kStopped;
+  return NS_OK;
+}
+
+nsresult
+MediaEngineDefaultVideoSource::Snapshot(PRUint32 aDuration, nsIDOMFile** aFile)
+{
+  *aFile = nsnull;
+
+#ifndef MOZ_WIDGET_ANDROID
+  return NS_ERROR_NOT_IMPLEMENTED;
+#else
+  if (!AndroidBridge::Bridge()) {
+    return NS_ERROR_UNEXPECTED;
+  }
+
+  nsAutoString filePath;
+  AndroidBridge::Bridge()->ShowFilePickerForMimeType(filePath, NS_LITERAL_STRING("image/*"));
+
+  nsCOMPtr<nsIFile> file;
+  nsresult rv = NS_NewLocalFile(filePath, false, getter_AddRefs(file));
+  NS_ENSURE_SUCCESS(rv, rv);
+
+  NS_ADDREF(*aFile = new nsDOMFileFile(file));
+  return NS_OK;
+#endif
+}
+
+NS_IMETHODIMP
+MediaEngineDefaultVideoSource::Notify(nsITimer* aTimer)
+{
+  VideoSegment segment;
+
+  nsRefPtr<layers::PlanarYCbCrImage> image = mImage;
+  segment.AppendFrame(image.forget(), USECS_PER_S / FPS, gfxIntSize(WIDTH, HEIGHT));
+  mSource->AppendToTrack(mTrackID, &segment);
+
+  return NS_OK;
+}
+
+NS_IMPL_THREADSAFE_ISUPPORTS1(MediaEngineDefaultAudioSource, nsITimerCallback)
+/**
+ * Default audio source.
+ */
+void
+MediaEngineDefaultAudioSource::GetName(nsAString& aName)
+{
+  aName.Assign(NS_LITERAL_STRING("Default Audio Device"));
+  return;
+}
+
+void
+MediaEngineDefaultAudioSource::GetUUID(nsAString& aUUID)
+{
+  aUUID.Assign(NS_LITERAL_STRING("B7CBD7C1-53EF-42F9-8353-73F61C70C092"));
+  return;
+}
+
+already_AddRefed<nsDOMMediaStream>
+MediaEngineDefaultAudioSource::Allocate()
+{
+  if (mState != kReleased) {
+    return NULL;
+  }
+  mState = kAllocated;
+  return nsDOMMediaStream::CreateInputStream();
+}
+
+nsresult
+MediaEngineDefaultAudioSource::Deallocate()
+{
+  if (mState != kStopped && mState != kAllocated) {
+    return NS_ERROR_FAILURE;
+  }
+  mState = kReleased;
+  return NS_OK;
+}
+
+nsresult
+MediaEngineDefaultAudioSource::Start(SourceMediaStream* aStream, TrackID aID)
+{
+  if (mState != kAllocated) {
+    return NULL;
+  }
+
+  mTimer = do_CreateInstance(NS_TIMER_CONTRACTID);
+  if (!mTimer) {
+    return NULL;
+  }
+
+  mSource = aStream;
+
+  // AddTrack will take ownership of segment
+  AudioSegment* segment = new AudioSegment();
+  segment->Init(CHANNELS);
+  mSource->AddTrack(aID, RATE, 0, segment);
+
+  // We aren't going to add any more tracks
+  mSource->AdvanceKnownTracksTime(STREAM_TIME_MAX);
+
+  // Remember TrackID so we can finish later
+  mTrackID = aID;
+
+  // 1 Audio frame per Video frame
+  mTimer->InitWithCallback(this, 1000 / FPS, nsITimer::TYPE_REPEATING_SLACK);
+  mState = kStarted;
+
+  return NS_OK;
+}
+
+nsresult
+MediaEngineDefaultAudioSource::Stop()
+{
+  if (mState != kStarted) {
+    return NS_ERROR_FAILURE;
+  }
+  if (!mTimer) {
+    return NS_ERROR_FAILURE;
+  }
+
+  mTimer->Cancel();
+  mTimer = NULL;
+
+  mSource->EndTrack(mTrackID);
+  mSource->Finish();
+
+  mState = kStopped;
+  return NS_OK;
+}
+
+nsresult
+MediaEngineDefaultAudioSource::Snapshot(PRUint32 aDuration, nsIDOMFile** aFile)
+{
+   return NS_ERROR_NOT_IMPLEMENTED;
+}
+
+NS_IMETHODIMP
+MediaEngineDefaultAudioSource::Notify(nsITimer* aTimer)
+{
+  AudioSegment segment;
+  segment.Init(CHANNELS);
+  segment.InsertNullDataAtStart(1);
+
+  mSource->AppendToTrack(mTrackID, &segment);
+
+  return NS_OK;
+}
+
+void
+MediaEngineDefault::EnumerateVideoDevices(nsTArray<nsRefPtr<MediaEngineVideoSource> >* aVSources) {
+  aVSources->AppendElement(mVSource);
+  return;
+}
+
+void
+MediaEngineDefault::EnumerateAudioDevices(nsTArray<nsRefPtr<MediaEngineAudioSource> >* aASources) {
+  aASources->AppendElement(mASource);
+  return;
+}
+
+} // namespace mozilla
diff --git a/content/media/webrtc/MediaEngineDefault.h b/content/media/webrtc/MediaEngineDefault.h
new file mode 100644
--- /dev/null
+++ b/content/media/webrtc/MediaEngineDefault.h
@@ -0,0 +1,115 @@
+/* This Source Code Form is subject to the terms of the Mozilla Public
+ * License, v. 2.0. If a copy of the MPL was not distributed with this file,
+ * You can obtain one at http://mozilla.org/MPL/2.0/. */
+
+#ifndef MEDIAENGINEDEFAULT_H_
+#define MEDIAENGINEDEFAULT_H_
+
+#include "prmem.h"
+#include "nsITimer.h"
+
+#include "nsCOMPtr.h"
+#include "nsDOMMediaStream.h"
+#include "nsComponentManagerUtils.h"
+
+#include "Layers.h"
+#include "VideoUtils.h"
+#include "MediaEngine.h"
+#include "ImageLayers.h"
+#include "VideoSegment.h"
+#include "AudioSegment.h"
+#include "StreamBuffer.h"
+#include "MediaStreamGraph.h"
+
+namespace mozilla {
+
+/**
+ * The default implementation of the MediaEngine interface.
+ */
+
+enum DefaultEngineState {
+  kAllocated,
+  kStarted,
+  kStopped,
+  kReleased
+};
+
+class MediaEngineDefaultVideoSource : public nsITimerCallback,
+                                      public MediaEngineVideoSource
+{
+public:
+  MediaEngineDefaultVideoSource() : mTimer(nsnull), mState(kReleased) {}
+  ~MediaEngineDefaultVideoSource(){};
+
+  virtual void GetName(nsAString&);
+  virtual void GetUUID(nsAString&);
+
+  virtual MediaEngineVideoOptions GetOptions();
+  virtual already_AddRefed<nsDOMMediaStream> Allocate();
+
+  virtual nsresult Deallocate();
+  virtual nsresult Start(SourceMediaStream*, TrackID);
+  virtual nsresult Stop();
+  virtual nsresult Snapshot(PRUint32 aDuration, nsIDOMFile** aFile);
+
+  NS_DECL_ISUPPORTS
+  NS_DECL_NSITIMERCALLBACK
+
+protected:
+  TrackID mTrackID;
+  nsCOMPtr<nsITimer> mTimer;
+  nsRefPtr<layers::ImageContainer> mImageContainer;
+
+  DefaultEngineState mState;
+  SourceMediaStream* mSource;
+  layers::PlanarYCbCrImage* mImage;
+};
+
+class MediaEngineDefaultAudioSource : public nsITimerCallback,
+                                      public MediaEngineAudioSource
+{
+public:
+  MediaEngineDefaultAudioSource() : mTimer(nsnull), mState(kReleased) {}
+  ~MediaEngineDefaultAudioSource(){};
+
+  virtual void GetName(nsAString&);
+  virtual void GetUUID(nsAString&);
+
+  virtual already_AddRefed<nsDOMMediaStream> Allocate();
+
+  virtual nsresult Deallocate();
+  virtual nsresult Start(SourceMediaStream*, TrackID);
+  virtual nsresult Stop();
+  virtual nsresult Snapshot(PRUint32 aDuration, nsIDOMFile** aFile);
+
+  NS_DECL_ISUPPORTS
+  NS_DECL_NSITIMERCALLBACK
+
+protected:
+  TrackID mTrackID;
+  nsCOMPtr<nsITimer> mTimer;
+
+  DefaultEngineState mState;
+  SourceMediaStream* mSource;
+};
+
+class MediaEngineDefault : public MediaEngine
+{
+public:
+  MediaEngineDefault() {
+    mVSource = new MediaEngineDefaultVideoSource();
+    mASource = new MediaEngineDefaultAudioSource();
+  }
+  ~MediaEngineDefault() {}
+
+  virtual void EnumerateVideoDevices(nsTArray<nsRefPtr<MediaEngineVideoSource> >*);
+  virtual void EnumerateAudioDevices(nsTArray<nsRefPtr<MediaEngineAudioSource> >*);
+
+private:
+  nsRefPtr<MediaEngineVideoSource> mVSource;
+  nsRefPtr<MediaEngineAudioSource> mASource;
+};
+
+}
+
+#endif /* NSMEDIAENGINEDEFAULT_H_ */
diff --git a/content/media/webrtc/MediaEngineWebRTC.h b/content/media/webrtc/MediaEngineWebRTC.h
new file mode 100644
--- /dev/null
+++ b/content/media/webrtc/MediaEngineWebRTC.h
@@ -0,0 +1,210 @@
+/* This Source Code Form is subject to the terms of the Mozilla Public
+ * License, v. 2.0. If a copy of the MPL was not distributed with this file,
+ * You can obtain one at http://mozilla.org/MPL/2.0/. */
+
+#ifndef MEDIAENGINEWEBRTC_H_
+#define MEDIAENGINEWEBRTC_H_
+
+#include "prmem.h"
+#include "prcvar.h"
+#include "nsITimer.h"
+#include "nsIThread.h"
+#include "nsIRunnable.h"
+
+#include "nsCOMPtr.h"
+#include "nsDOMMediaStream.h"
+#include "nsComponentManagerUtils.h"
+#include "nsDOMFile.h"
+
+#include "prthread.h"
+#include "nsThreadUtils.h"
+#include "Layers.h"
+#include "VideoUtils.h"
+#include "MediaEngine.h"
+#include "ImageLayers.h"
+#include "VideoSegment.h"
+#include "AudioSegment.h"
+#include "StreamBuffer.h"
+#include "MediaStreamGraph.h"
+
+// WebRTC library includes follow
+
+// Audio Engine
+#include "voice_engine/main/interface/voe_base.h"
+#include "voice_engine/main/interface/voe_hardware.h"
+#include "voice_engine/main/interface/voe_audio_processing.h"
+#include "voice_engine/main/interface/voe_volume_control.h"
+#include "voice_engine/main/interface/voe_external_media.h"
+
+// Video Engine
+#include "video_engine/include/vie_base.h"
+#include "video_engine/include/vie_codec.h"
+#include "video_engine/include/vie_render.h"
+#include "video_engine/include/vie_capture.h"
+#include "video_engine/include/vie_file.h"
+
+
+namespace mozilla {
+
+/**
+ * The WebRTC implementation of the MediaEngine interface.
+ */
+
+enum WebrtcEngineState {
+  kAllocated,
+  kStarted,
+  kStopped,
+  kReleased,
+};
+
+class MediaEngineWebRTCVideoSource : public MediaEngineVideoSource, public webrtc::ExternalRenderer 
+{
+public:
+  // ViEExternalRenderer.
+  virtual int FrameSizeChange(unsigned int, unsigned int, unsigned int);
+  virtual int DeliverFrame(unsigned char*, int, uint32_t, int64_t);
+
+  MediaEngineWebRTCVideoSource(webrtc::VideoEngine* videoEnginePtr,
+    int index, int aFps = 30)
+    : mVideoEngine(videoEnginePtr)
+    , mCapIndex(index)
+    , mWidth(640)
+    , mHeight(480)
+    , mState(kReleased)
+    , mMonitor("WebRTCCamera.Monitor")
+    , mFps(aFps)
+    , mInitDone(false)
+    , mInSnapshotMode(false) { Init(); }
+
+  ~MediaEngineWebRTCVideoSource() { Shutdown(); }
+
+  virtual void GetName(nsAString&);
+  virtual void GetUUID(nsAString&);
+  virtual MediaEngineVideoOptions GetOptions();
+  virtual already_AddRefed<nsDOMMediaStream> Allocate();
+  virtual nsresult Deallocate();
+  virtual nsresult Start(SourceMediaStream*, TrackID);
+  virtual nsresult Stop();
+  virtual nsresult Snapshot(PRUint32 aDuration, nsIDOMFile** aFile);
+
+  NS_DECL_ISUPPORTS
+
+private:
+  static const unsigned int KMaxDeviceNameLength;
+  static const unsigned int KMaxUniqueIdLength;
+
+  // Initialize the needed Video engine interfaces.
+  void Init();
+  void Shutdown();
+
+  // Engine variables.
+
+  webrtc::VideoEngine* mVideoEngine; // Weak reference, don't free.
+  webrtc::ViEBase* mViEBase;
+  webrtc::ViECapture* mViECapture;
+  webrtc::ViERender* mViERender;
+  webrtc::CaptureCapability mCaps; // Doesn't work on OS X.
+
+  int mCapIndex;
+  int mWidth, mHeight;
+  TrackID mTrackID;
+
+  WebrtcEngineState mState;
+  mozilla::ReentrantMonitor mMonitor; // Monitor for processing WebRTC frames.
+  SourceMediaStream* mSource;
+
+  int mFps; // Track rate (30 fps by default)
+  bool mInitDone;
+  bool mInSnapshotMode;
+  nsRefPtr<layers::ImageContainer> mImageContainer;
+
+  PRLock* mSnapshotLock;
+  PRCondVar* mSnapshotCondVar;
+};
+
+class MediaEngineWebRTCAudioSource : public nsITimerCallback,
+                                     public MediaEngineAudioSource
+{
+public:
+  static const unsigned int PLAYOUT_SAMPLE_FREQUENCY; // Default is 16000.
+  static const unsigned int PLAYOUT_SAMPLE_LENGTH; // Default is 160.
+
+  MediaEngineWebRTCAudioSource(webrtc::VoiceEngine* voiceEngine,  int aIndex)
+  : mTimer(nsnull)
+  , mVoiceEngine(voiceEngine)
+  , mMonitor("WebRTCMic.Monitor")
+  , mCapIndex(aIndex)
+  , mChannel(-1)
+  , mInitDone(false)
+  , mState(kReleased) { Init(); }
+
+  ~MediaEngineWebRTCAudioSource() { Shutdown(); }
+
+  virtual void GetName(nsAString&);
+  virtual void GetUUID(nsAString&);
+
+  virtual already_AddRefed<nsDOMMediaStream> Allocate();
+  virtual nsresult Deallocate();
+  virtual nsresult Start(SourceMediaStream*, TrackID);
+  virtual nsresult Stop();
+  virtual nsresult Snapshot(PRUint32 aDuration, nsIDOMFile** aFile);
+
+  NS_DECL_ISUPPORTS
+  NS_DECL_NSITIMERCALLBACK
+
+protected:
+  nsCOMPtr<nsITimer> mTimer; // 10ms audio sample timer.
+
+private:
+  static const unsigned int KMaxDeviceNameLength;
+  static const unsigned int KMaxUniqueIdLength;
+
+  void Init();
+  void Shutdown();
+
+  webrtc::VoiceEngine* mVoiceEngine;
+  webrtc::VoEBase* mVoEBase;
+  webrtc::VoEHardware* mVoEHw;
+  webrtc::VoEExternalMedia* mVoEXmedia; // For external playout.
+
+  mozilla::ReentrantMonitor mMonitor;
+
+  int mCapIndex;
+  int mChannel;
+  TrackID mTrackID;
+  bool mInitDone;
+  WebrtcEngineState mState;
+  SourceMediaStream* mSource;
+  AudioSegment mAudioSegment;
+};
+
+class MediaEngineWebRTC : public MediaEngine
+{
+public:
+  MediaEngineWebRTC()
+  : mVideoEngine(NULL)
+  , mVoiceEngine(NULL)
+  , mVideoEngineInit(false)
+  , mAudioEngineInit(false) {}
+
+  ~MediaEngineWebRTC() { Shutdown(); }
+
+  // Clients should ensure to clean-up sources video/audio sources
+  // before invoking Shutdown on this class.
+  void Shutdown();
+
+  virtual void EnumerateVideoDevices(nsTArray<nsRefPtr<MediaEngineVideoSource> >*);
+  virtual void EnumerateAudioDevices(nsTArray<nsRefPtr<MediaEngineAudioSource> >*);
+
+private:
+  webrtc::VideoEngine* mVideoEngine;
+  webrtc::VoiceEngine* mVoiceEngine;
+
+  // Need this to avoid unneccesary WebRTC calls while enumerating.
+  bool mVideoEngineInit;
+  bool mAudioEngineInit;
+};
+
+}
+
+#endif /* NSMEDIAENGINEWEBRTC_H_ */
diff --git a/content/media/webrtc/MediaEngineWebRTCAudio.cpp b/content/media/webrtc/MediaEngineWebRTCAudio.cpp
new file mode 100644
--- /dev/null
+++ b/content/media/webrtc/MediaEngineWebRTCAudio.cpp
@@ -0,0 +1,763 @@
+/* This Source Code Form is subject to the terms of the Mozilla Public
+ * License, v. 2.0. If a copy of the MPL was not distributed with this file,
+ * You can obtain one at http://mozilla.org/MPL/2.0/. */
+
+#include "MediaEngineWebRTC.h"
+
+#define CHANNELS 1
+
+namespace mozilla {
+
+/**
+ * Webrtc video source.
+ */
+NS_IMPL_THREADSAFE_ISUPPORTS0(MediaEngineWebRTCVideoSource)
+
+// Static variables to hold device names and UUIDs.
+const unsigned int MediaEngineWebRTCVideoSource::KMaxDeviceNameLength = 128;
+const unsigned int MediaEngineWebRTCVideoSource::KMaxUniqueIdLength = 256;
+
+// ViEExternalRenderer Callback.
+int
+MediaEngineWebRTCVideoSource::FrameSizeChange(
+   unsigned int w, unsigned int h, unsigned int streams)
+{
+  mWidth = w;
+  mHeight = h;
+  return 0;
+}
+
+// ViEExternalRenderer Callback. Process every incoming frame here.
+int
+MediaEngineWebRTCVideoSource::DeliverFrame(
+   unsigned char* buffer, int size, uint32_t time_stamp, int64_t render_time)
+{
+  ReentrantMonitorAutoEnter enter(mMonitor);
+
+  if (mInSnapshotMode) {
+    // Set the condition variable to false and notify Snapshot().
+    PR_Lock(mSnapshotLock);
+    mInSnapshotMode = false;
+    PR_NotifyCondVar(mSnapshotCondVar);
+    PR_Unlock(mSnapshotLock);
+    return 0;
+  }
+
+  // Check for proper state.
+  if (mState != kStarted) {
+    return 0;
+  }
+
+  // Create a video frame and append it to the track.
+  layers::Image::Format format = layers::Image::PLANAR_YCBCR;
+  nsRefPtr<layers::Image> image = mImageContainer->CreateImage(&format, 1);
+
+  layers::PlanarYCbCrImage* videoImage = static_cast<layers::PlanarYCbCrImage*>(image.get());
+
+  PRUint8* frame = static_cast<PRUint8*> (buffer);
+  const PRUint8 lumaBpp = 8;
+  const PRUint8 chromaBpp = 4;
+
+  layers::PlanarYCbCrImage::Data data;
+  data.mYChannel = frame;
+  data.mYSize = gfxIntSize(mWidth, mHeight);
+  data.mYStride = mWidth * lumaBpp/ 8;
+  data.mCbCrStride = mWidth * chromaBpp / 8;
+  data.mCbChannel = frame + mHeight * data.mYStride;
+  data.mCrChannel = data.mCbChannel + mHeight * data.mCbCrStride / 2;
+  data.mCbCrSize = gfxIntSize(mWidth/ 2, mHeight/ 2);
+  data.mPicX = 0;
+  data.mPicY = 0;
+  data.mPicSize = gfxIntSize(mWidth, mHeight);
+  data.mStereoMode = layers::STEREO_MODE_MONO;
+
+  videoImage->SetData(data);
+
+  VideoSegment segment;
+  segment.AppendFrame(image.forget(), 1, gfxIntSize(mWidth, mHeight));
+  mSource->AppendToTrack(mTrackID, &(segment));
+  return 0;
+}
+
+void
+MediaEngineWebRTCVideoSource::GetName(nsAString& aName)
+{
+  char deviceName[KMaxDeviceNameLength];
+  memset(deviceName, 0, KMaxDeviceNameLength);
+
+  char uniqueId[KMaxUniqueIdLength];
+  memset(uniqueId, 0, KMaxUniqueIdLength);
+
+  if (mInitDone) {
+    mViECapture->GetCaptureDevice(
+      mCapIndex, deviceName, KMaxDeviceNameLength, uniqueId, KMaxUniqueIdLength
+    );
+    aName.Assign(NS_ConvertASCIItoUTF16(deviceName));
+  }
+}
+
+void
+MediaEngineWebRTCVideoSource::GetUUID(nsAString& aUUID)
+{
+  char deviceName[KMaxDeviceNameLength];
+  memset(deviceName, 0, KMaxDeviceNameLength);
+
+  char uniqueId[KMaxUniqueIdLength];
+  memset(uniqueId, 0, KMaxUniqueIdLength);
+
+  if (mInitDone) {
+    mViECapture->GetCaptureDevice(
+      mCapIndex, deviceName, KMaxDeviceNameLength, uniqueId, KMaxUniqueIdLength
+    );
+    aUUID.Assign(NS_ConvertASCIItoUTF16(uniqueId));
+  }
+}
+
+already_AddRefed<nsDOMMediaStream>
+MediaEngineWebRTCVideoSource::Allocate()
+{
+  if (mState != kReleased) {
+    return NULL;
+  }
+
+  char deviceName[KMaxDeviceNameLength];
+  memset(deviceName, 0, KMaxDeviceNameLength);
+
+  char uniqueId[KMaxUniqueIdLength];
+  memset(uniqueId, 0, KMaxUniqueIdLength);
+
+  mViECapture->GetCaptureDevice(
+    mCapIndex, deviceName, KMaxDeviceNameLength, uniqueId, KMaxUniqueIdLength
+  );
+
+  if (mViECapture->AllocateCaptureDevice(uniqueId, KMaxUniqueIdLength, mCapIndex) == 0) {
+    mState = kAllocated;
+    if (mViECapture->StartCapture(mCapIndex) == -1) {
+      return NULL;
+    }
+    return nsDOMMediaStream::CreateInputStream();
+  }
+
+  return NULL;
+}
+
+nsresult
+MediaEngineWebRTCVideoSource::Deallocate()
+{
+  if (mState != kStopped && mState != kAllocated) {
+    return NS_ERROR_FAILURE;
+  }
+
+  mViECapture->StopCapture(mCapIndex);
+  mViECapture->ReleaseCaptureDevice(mCapIndex);
+  mState = kReleased;
+  return NS_OK;
+}
+
+MediaEngineVideoOptions
+MediaEngineWebRTCVideoSource::GetOptions()
+{
+  MediaEngineVideoOptions aOpts;
+  aOpts.mWidth = mWidth;
+  aOpts.mHeight = mHeight;
+  aOpts.mMaxFPS = mFps;
+  aOpts.codecType = kVideoCodecI420;
+  return aOpts;
+}
+
+nsresult
+MediaEngineWebRTCVideoSource::Start(SourceMediaStream* aStream, TrackID aID)
+{
+  int error = 0;
+  if (!mInitDone || mState != kAllocated) {
+    return NS_ERROR_FAILURE;
+  }
+
+  if (!aStream) {
+    return NS_ERROR_FAILURE;
+  }
+
+  if (mState == kStarted) {
+    return NS_OK;
+  }
+
+  mSource = aStream;
+  mTrackID = aID;
+
+  // Setup  a blank track.
+  mImageContainer = layers::LayerManager::CreateImageContainer();
+  mSource->AddTrack(aID, mFps, 0, new VideoSegment());
+  mSource->AdvanceKnownTracksTime(STREAM_TIME_MAX);
+
+  error = mViERender->AddRenderer(mCapIndex, webrtc::kVideoI420, (webrtc::ExternalRenderer*)this);
+  if (error == -1) {
+    return NS_ERROR_FAILURE;
+  }
+
+  error = mViERender->StartRender(mCapIndex);
+  if (error == -1) {
+    return NS_ERROR_FAILURE;
+  }
+
+  mState = kStarted;
+  return NS_OK;
+}
+
+nsresult
+MediaEngineWebRTCVideoSource::Stop()
+{
+  if (mState != kStarted) {
+    return NS_ERROR_FAILURE;
+  }
+
+  mSource->EndTrack(mTrackID);
+  mSource->Finish();
+
+  mViERender->StopRender(mCapIndex);
+  mViERender->RemoveRenderer(mCapIndex);
+
+  mState = kStopped;
+  return NS_OK;
+}
+
+nsresult
+MediaEngineWebRTCVideoSource::Snapshot(PRUint32 aDuration, nsIDOMFile** aFile)
+{
+  /**
+   * To get a Snapshot we do the following:
+   * - Set a condition variable (mInSnapshotMode) to true
+   * - Attach the external renderer and start the camera
+   * - Wait for the condition variable to change to false
+   *
+   * Starting the camera has the effect of invoking DeliverFrame() when
+   * the first frame arrives from the camera. We only need one frame for
+   * GetCaptureDeviceSnapshot to work, so we immediately set the condition
+   * variable to false and notify this method.
+   *
+   * This causes the current thread to continue (PR_CondWaitVar will return),
+   * at which point we can grab a snapshot, convert it to a file and
+   * return from this function after cleaning up the temporary stream object
+   * and caling Stop() on the media source.
+   */
+  *aFile = nsnull;
+  if (!mInitDone || mState != kAllocated) {
+    return NS_ERROR_FAILURE;
+  }
+
+  mSnapshotLock = PR_NewLock();
+  mSnapshotCondVar = PR_NewCondVar(mSnapshotLock);
+
+  PR_Lock(mSnapshotLock);
+  mInSnapshotMode = true;
+  nsRefPtr<nsDOMMediaStream> stream = nsDOMMediaStream::CreateInputStream();
+
+  // Start the rendering (equivalent to calling Start(), but without a track).
+  int error = 0;
+  if (!mInitDone || mState != kAllocated) {
+    return NS_ERROR_FAILURE;
+  }
+  error = mViERender->AddRenderer(mCapIndex, webrtc::kVideoI420, (webrtc::ExternalRenderer*)this);
+  if (error == -1) {
+    return NS_ERROR_FAILURE;
+  }
+  error = mViERender->StartRender(mCapIndex);
+  if (error == -1) {
+    return NS_ERROR_FAILURE;
+  }
+
+  // Wait for the condition variable, will be set in DeliverFrame.
+  // We use a while loop, because even if PR_WaitCondVar returns, it's not
+  // guaranteed that the condition variable changed.
+  while (mInSnapshotMode) {
+    PR_WaitCondVar(mSnapshotCondVar, PR_INTERVAL_NO_TIMEOUT);
+  }
+
+  // If we get here, DeliverFrame received at least one frame.
+  PR_Unlock(mSnapshotLock);
+  PR_DestroyCondVar(mSnapshotCondVar);
+  PR_DestroyLock(mSnapshotLock);
+
+  webrtc::ViEFile* vieFile = webrtc::ViEFile::GetInterface(mVideoEngine);
+  if (!vieFile) {
+    return NS_ERROR_FAILURE;
+  }
+
+  // Create a temporary file and put the snapshot in it.
+  nsCOMPtr<nsIFile> tmp;
+  nsresult rv = NS_GetSpecialDirectory("TmpD", getter_AddRefs(tmp));
+  NS_ENSURE_SUCCESS(rv, rv);
+
+  tmp->Append(NS_LITERAL_STRING("webrtc_snapshot.jpeg"));
+  rv = tmp->CreateUnique(nsIFile::NORMAL_FILE_TYPE, 0600);
+  NS_ENSURE_SUCCESS(rv, rv);
+
+  nsString unicodePath;
+  rv = tmp->GetPath(unicodePath);
+  NS_ENSURE_SUCCESS(rv, rv);
+
+  const char* path = NS_ConvertUTF16toUTF8(unicodePath).get();
+  if (vieFile->GetCaptureDeviceSnapshot(mCapIndex, path) < 0) {
+    return NS_ERROR_FAILURE;
+  }
+
+  // Stop the camera.
+  mViERender->StopRender(mCapIndex);
+  mViERender->RemoveRenderer(mCapIndex);
+
+  NS_ADDREF(*aFile = new nsDOMFileFile(tmp));
+
+  return NS_OK;
+}
+
+/**
+ * Initialization and Shutdown functions for the video source, called by the
+ * constructor and destructor respectively.
+ */
+
+void
+MediaEngineWebRTCVideoSource::Init()
+{
+  if (mVideoEngine == NULL) {
+    return;
+  }
+
+  mViEBase = webrtc::ViEBase::GetInterface(mVideoEngine);
+  if (mViEBase == NULL) {
+    return;
+  }
+
+  // Get interfaces for capture, render for now
+  mViECapture = webrtc::ViECapture::GetInterface(mVideoEngine);
+  mViERender = webrtc::ViERender::GetInterface(mVideoEngine);
+
+  if (mViECapture == NULL || mViERender == NULL) {
+    return;
+  }
+
+  // Temporary logging
+  mVideoEngine->SetTraceFilter(webrtc::kTraceAll);
+  mVideoEngine->SetTraceFile("Vievideotrace.out");
+
+  mInitDone = true;
+}
+
+void
+MediaEngineWebRTCVideoSource::Shutdown()
+{
+  int error = 0;
+  bool continueShutdown = false;
+
+  if (!mInitDone) {
+    return;
+  }
+
+  if (mState == kStarted) {
+    error = mViERender->StopRender(mCapIndex);
+    error = mViERender->RemoveRenderer(mCapIndex);
+    continueShutdown = true;
+  }
+
+  error = 0;
+  if (mState == kAllocated || continueShutdown) {
+    error = mViECapture->StopCapture(mCapIndex);
+    error = mViECapture->ReleaseCaptureDevice(mCapIndex);
+    continueShutdown = false;
+  }
+
+  mViECapture->Release();
+  mViERender->Release();
+  mViEBase->Release();
+  mState = kReleased;
+  mInitDone = false;
+}
+
+
+
+/**
+ * Webrtc audio source.
+ */
+
+NS_IMPL_THREADSAFE_ISUPPORTS1(MediaEngineWebRTCAudioSource, nsITimerCallback)
+
+//static initialization
+const unsigned int MediaEngineWebRTCAudioSource::PLAYOUT_SAMPLE_FREQUENCY = 16000;
+const unsigned int MediaEngineWebRTCAudioSource::PLAYOUT_SAMPLE_LENGTH  = 160;
+const unsigned int MediaEngineWebRTCAudioSource::KMaxDeviceNameLength = 128;
+const unsigned int MediaEngineWebRTCAudioSource::KMaxUniqueIdLength = 128;
+
+// Performs very basic & common initialization
+void
+MediaEngineWebRTCAudioSource::Init()
+{
+  if(NULL == mVoiceEngine)
+   return;
+
+  mVoEBase = webrtc::VoEBase::GetInterface(mVoiceEngine);
+  if (NULL == mVoEBase )
+  {
+    printf( "ERROR in AudioEngine::VoEBase ");
+    return;
+  }
+
+  //get interfaces for capture, render for now
+  mVoEHw = webrtc::VoEHardware::GetInterface(mVoiceEngine);
+
+  // check if all the interfaces were ok till now
+  if(NULL == mVoEHw)
+   return ;
+
+  mChannel = mVoEBase->CreateChannel();
+  if(-1 == mChannel)
+  {
+ return;
+  }
+
+  mVoEXmedia = webrtc::VoEExternalMedia::GetInterface(mVoiceEngine);
+  if(NULL == mVoEXmedia)
+  {
+    return;
+  }
+  //temp logging
+  mVoiceEngine->SetTraceFilter(webrtc::kTraceAll);
+  mVoiceEngine->SetTraceFile( "Voevideotrace.out" );
+  mAudioSegment.Init(CHANNELS);
+
+  // we should be good by now
+  mInitDone = true;
+
+}
+
+void
+MediaEngineWebRTCAudioSource::GetName(nsAString& aName)
+{
+  char deviceName[KMaxDeviceNameLength];
+  memset(deviceName, 0, KMaxDeviceNameLength);
+  char uniqueId[KMaxUniqueIdLength];
+  memset(uniqueId, 0, KMaxUniqueIdLength);
+  if(true == mInitDone)
+  {
+    mVoEHw->GetRecordingDeviceName(
+             mCapIndex, deviceName,  uniqueId );
+   aName.Assign(NS_ConvertASCIItoUTF16(deviceName));
+  }
+
+  return;
+}
+
+void
+MediaEngineWebRTCAudioSource::GetUUID(nsAString& aUUID)
+{
+  char deviceName[KMaxDeviceNameLength];
+  memset(deviceName, 0, KMaxDeviceNameLength);
+  char uniqueId[KMaxUniqueIdLength];
+  memset(uniqueId, 0, KMaxUniqueIdLength);
+  if(true == mInitDone )
+  {
+    mVoEHw->GetRecordingDeviceName(
+             mCapIndex, deviceName,  uniqueId );
+   aUUID.Assign(NS_ConvertASCIItoUTF16(uniqueId));
+  }
+
+  return;
+}
+
+already_AddRefed<nsDOMMediaStream>
+MediaEngineWebRTCAudioSource::Allocate()
+{
+  if (mState != kReleased) {
+    return NULL;
+  }
+  // no special webrtc code to be done , i hope
+  mState = kAllocated;
+  return nsDOMMediaStream::CreateInputStream();
+}
+
+nsresult
+MediaEngineWebRTCAudioSource::Deallocate()
+{
+  if (mState != kStopped && mState != kAllocated) {
+    return NS_ERROR_FAILURE;
+  }
+  mState = kReleased;
+  return NS_OK;
+}
+
+//Loop back audio through media-stream
+nsresult
+MediaEngineWebRTCAudioSource::Start(SourceMediaStream* aStream, TrackID aID)
+{
+  const int DEFAULT_PORT = 55555;
+  printf("\n MediaEngineWebRTCAudioSource : Start: Entered ");
+  if (false == mInitDone || mState != kAllocated) {
+    return NS_ERROR_FAILURE;
+  }
+
+  if(!aStream)
+   return NS_ERROR_FAILURE;
+
+  mTimer = do_CreateInstance(NS_TIMER_CONTRACTID);
+  if (!mTimer) {
+    return NS_ERROR_FAILURE;
+  }
+
+  mSource = aStream;
+
+  AudioSegment* segment = new AudioSegment();
+  segment->Init(CHANNELS);
+  //segment->InsertNullDataAtStart(1);
+  mSource->AddTrack(aID, PLAYOUT_SAMPLE_FREQUENCY, 0, segment);
+  mSource->AdvanceKnownTracksTime(STREAM_TIME_MAX);
+  mTrackID = aID;
+
+  printf("\n Starting the audio engine ");
+  mVoEBase->SetLocalReceiver(mChannel,DEFAULT_PORT);
+  mVoEBase->SetSendDestination(mChannel,DEFAULT_PORT,"127.0.0.1");
+
+  if(-1 == mVoEXmedia->SetExternalPlayoutStatus(true)) {
+    printf("\n SetExternalPlayoutStatus failed %d ", mVoEBase->LastError() );
+   return NS_ERROR_FAILURE;
+  }
+  //loopback audio
+  mVoEBase->StartPlayout(mChannel);
+  mVoEBase->StartReceive(mChannel);
+  mVoEBase->StartSend(mChannel);
+
+  mState = kStarted;
+  // call every 10 milliseconds
+  mTimer->InitWithCallback(this, 10, nsITimer::TYPE_REPEATING_SLACK);
+  return NS_OK;
+}
+
+nsresult
+MediaEngineWebRTCAudioSource::Stop()
+{
+  if (mState != kStarted) {
+    return NS_ERROR_FAILURE;
+  }
+  if (!mTimer) {
+    return NS_ERROR_FAILURE;
+  }
+
+  if(!mVoEBase) {
+    return NS_ERROR_FAILURE;
+  }
+
+  mVoEBase->StopReceive(mChannel);
+  mVoEBase->StopSend(mChannel);
+  mVoEBase->StopPlayout(mChannel);
+
+  mTimer->Cancel();
+  mTimer = NULL;
+  mState = kStopped;
+  return NS_OK;
+}
+
+nsresult
+MediaEngineWebRTCAudioSource::Snapshot(PRUint32 aDuration, nsIDOMFile** aFile)
+{
+   return NS_ERROR_NOT_IMPLEMENTED;
+}
+
+
+void
+MediaEngineWebRTCAudioSource::Shutdown()
+{
+  if(false == mInitDone)
+   return;
+
+  if(kStarted == mState)
+  {
+   //Stop External playout
+   mVoEBase->StopReceive(mChannel);
+   mVoEBase->StopSend(mChannel);
+   mVoEBase->StopPlayout(mChannel);
+   mTimer->Cancel();
+   mTimer = NULL;
+  }
+
+  mVoEBase->Terminate();
+  mVoEXmedia->Release();
+  mVoEHw->Release();
+  mVoEBase->Release();
+  mState = kReleased;
+  mInitDone = false;
+}
+
+
+NS_IMETHODIMP
+MediaEngineWebRTCAudioSource::Notify(nsITimer* aTimer)
+{
+  // just one audio sample
+  static int16_t audio10ms[PLAYOUT_SAMPLE_LENGTH];
+  int sample_length  =0;
+  memset(audio10ms, 0, PLAYOUT_SAMPLE_LENGTH * sizeof(short));
+  mVoEXmedia->ExternalPlayoutGetData(audio10ms, PLAYOUT_SAMPLE_FREQUENCY, 100, sample_length); 
+  if(sample_length == 0)
+  {
+ return NS_OK;
+  }
+
+  // allocate shared buffer of lenght bytes
+  int buff_size = PLAYOUT_SAMPLE_LENGTH * sizeof(short);
+  nsRefPtr<SharedBuffer> buffer = SharedBuffer::Create(buff_size);
+  //buffer->AddRef();
+  int16_t* dest = static_cast<int16_t*>(buffer->Data());
+  for(int i=0; i < sample_length; i++)
+  {
+ dest[i] = audio10ms[i];
+  }
+
+  mAudioSegment.AppendFrames(buffer.forget(), sample_length, 0, sample_length, nsAudioStream::FORMAT_S16_LE);
+  mSource->AppendToTrack(mTrackID, &mAudioSegment);
+  return NS_OK;
+}
+
+//XXX: Not multi-threded and non-renterant -???
+void
+MediaEngineWebRTC::EnumerateVideoDevices(nsTArray<nsRefPtr<MediaEngineVideoSource> >* aVSources)
+{
+  int error = 0;
+  printf("\n EnumerateVideo Devices ");
+
+  //we can use it function locals .. no need to save it in the class
+  webrtc::ViEBase* ptrViEBase;
+  webrtc::ViECapture* ptrViECapture;
+
+  if(NULL == mVideoEngine)
+  {
+      mVideoEngine = webrtc::VideoEngine::Create();
+      if (NULL == mVideoEngine )
+ {
+         printf( "ERROR in VideoEngine::Create\n");
+        return;
+     }
+  }
+
+  // te should have VideoEngine Created here
+  ptrViEBase = webrtc::ViEBase::GetInterface(mVideoEngine);
+  if (NULL == ptrViEBase) {
+    return;
+  }
+
+
+  if( false == mVideoEngineInit)
+  {
+   error = ptrViEBase->Init();
+   if (-1 == error ) {
+      printf( "ERROR in VideoEngine::Init\n");
+      return;
+   }
+    mVideoEngineInit = true;
+  }
+
+  ptrViECapture = webrtc::ViECapture::GetInterface(mVideoEngine);
+  if (NULL == ptrViECapture ) {
+    printf("ERROR in ViECapture::GetInterface\n");
+    return;
+  }
+
+  int num = ptrViECapture->NumberOfCaptureDevices();
+  if (num <= 0) {
+    printf( "ERROR no video devices found\n");
+    return;
+  } else {
+    printf("GetUserMedia:: Found %d devices!\n", num);
+  }
+
+  for(int i=0; i < num; i++)
+  {
+    //let's create VideoSouce objects
+    nsRefPtr<MediaEngineVideoSource> vSource = new MediaEngineWebRTCVideoSource(mVideoEngine , i);
+    aVSources->AppendElement( vSource );
+  }
+
+  // safe to release local interfaces on the engine
+   ptrViEBase->Release();
+   ptrViECapture->Release();
+
+  return;
+}
+
+void
+MediaEngineWebRTC::EnumerateAudioDevices(nsTArray<nsRefPtr<MediaEngineAudioSource> >* aASources)
+{
+  // Be sure to release these at the end of the function
+  // safety measure though
+  webrtc::VoEBase* ptrVoEBase = NULL;
+  webrtc::VoEHardware* ptrVoEHw = NULL;
+  int error = 0;
+
+  if(NULL == mVoiceEngine)
+  {
+    mVoiceEngine = webrtc::VoiceEngine::Create();
+    if(NULL == mVoiceEngine)
+    {
+ printf(" Unable to create voice engine ");
+ return;
+    }
+  }
+
+  // all interfaces pointers are ref-counted
+  // we should be good on mutiple calls to this function ..
+  ptrVoEBase = webrtc::VoEBase::GetInterface( mVoiceEngine );
+  if(NULL == ptrVoEBase)
+  {
+ printf(" VoEBase creation failed ");
+ return;
+  }
+
+  //Init the voice library for the common parts
+  if(false == mAudioEngineInit)
+  {
+ error = ptrVoEBase->Init();
+ if(-1 == error)
+ {
+   printf("\n Audio Engine Init Failed ");
+ }
+ mAudioEngineInit = true;
+  }
+
+  ptrVoEHw = webrtc::VoEHardware::GetInterface(mVoiceEngine);
+  if(NULL == ptrVoEHw)
+  {
+ printf("\n Unable to get Audio Hardware Interface ");
+    return;
+  }
+
+  //let's enumerate audio input devices
+  int nDevices = 0;
+  error = ptrVoEHw->GetNumOfRecordingDevices(nDevices);
+  printf("\n Number if devices is %d ",nDevices);
+  for(int i = 0; i < nDevices; i++)
+  {
+     char deviceName[128];
+   memset(deviceName, 0, 128);
+   char uniqueId[128];
+   memset(uniqueId, 0, 128);
+     ptrVoEHw->GetRecordingDeviceName(i, deviceName,  uniqueId );
+        nsRefPtr<MediaEngineAudioSource> aSource = new MediaEngineWebRTCAudioSource(mVoiceEngine , i);
+ aSource->AddRef();
+   aASources->AppendElement( aSource );
+  }
+
+  // releasing them to re-set the reference count ...
+  ptrVoEHw->Release();
+  ptrVoEBase->Release();
+}
+
+
+void
+MediaEngineWebRTC::Shutdown()
+{
+
+  if(mVideoEngine)
+    webrtc::VideoEngine::Delete(mVideoEngine);
+
+  if(mVoiceEngine)
+   webrtc::VoiceEngine::Delete(mVoiceEngine);
+
+  mVideoEngine = NULL;
+  mVoiceEngine = NULL;
+}
+
+}
diff --git a/content/media/webrtc/MediaEngineWebRTCVideo.cpp b/content/media/webrtc/MediaEngineWebRTCVideo.cpp
new file mode 100644
--- /dev/null
+++ b/content/media/webrtc/MediaEngineWebRTCVideo.cpp
@@ -0,0 +1,763 @@
+/* This Source Code Form is subject to the terms of the Mozilla Public
+ * License, v. 2.0. If a copy of the MPL was not distributed with this file,
+ * You can obtain one at http://mozilla.org/MPL/2.0/. */
+
+#include "MediaEngineWebRTC.h"
+
+#define CHANNELS 1
+
+namespace mozilla {
+
+/**
+ * Webrtc video source.
+ */
+NS_IMPL_THREADSAFE_ISUPPORTS0(MediaEngineWebRTCVideoSource)
+
+// Static variables to hold device names and UUIDs.
+const unsigned int MediaEngineWebRTCVideoSource::KMaxDeviceNameLength = 128;
+const unsigned int MediaEngineWebRTCVideoSource::KMaxUniqueIdLength = 256;
+
+// ViEExternalRenderer Callback.
+int
+MediaEngineWebRTCVideoSource::FrameSizeChange(
+   unsigned int w, unsigned int h, unsigned int streams)
+{
+  mWidth = w;
+  mHeight = h;
+  return 0;
+}
+
+// ViEExternalRenderer Callback. Process every incoming frame here.
+int
+MediaEngineWebRTCVideoSource::DeliverFrame(
+   unsigned char* buffer, int size, uint32_t time_stamp, int64_t render_time)
+{
+  ReentrantMonitorAutoEnter enter(mMonitor);
+
+  if (mInSnapshotMode) {
+    // Set the condition variable to false and notify Snapshot().
+    PR_Lock(mSnapshotLock);
+    mInSnapshotMode = false;
+    PR_NotifyCondVar(mSnapshotCondVar);
+    PR_Unlock(mSnapshotLock);
+    return 0;
+  }
+
+  // Check for proper state.
+  if (mState != kStarted) {
+    return 0;
+  }
+
+  // Create a video frame and append it to the track.
+  layers::Image::Format format = layers::Image::PLANAR_YCBCR;
+  nsRefPtr<layers::Image> image = mImageContainer->CreateImage(&format, 1);
+
+  layers::PlanarYCbCrImage* videoImage = static_cast<layers::PlanarYCbCrImage*>(image.get());
+
+  PRUint8* frame = static_cast<PRUint8*> (buffer);
+  const PRUint8 lumaBpp = 8;
+  const PRUint8 chromaBpp = 4;
+
+  layers::PlanarYCbCrImage::Data data;
+  data.mYChannel = frame;
+  data.mYSize = gfxIntSize(mWidth, mHeight);
+  data.mYStride = mWidth * lumaBpp/ 8;
+  data.mCbCrStride = mWidth * chromaBpp / 8;
+  data.mCbChannel = frame + mHeight * data.mYStride;
+  data.mCrChannel = data.mCbChannel + mHeight * data.mCbCrStride / 2;
+  data.mCbCrSize = gfxIntSize(mWidth/ 2, mHeight/ 2);
+  data.mPicX = 0;
+  data.mPicY = 0;
+  data.mPicSize = gfxIntSize(mWidth, mHeight);
+  data.mStereoMode = layers::STEREO_MODE_MONO;
+
+  videoImage->SetData(data);
+
+  VideoSegment segment;
+  segment.AppendFrame(image.forget(), 1, gfxIntSize(mWidth, mHeight));
+  mSource->AppendToTrack(mTrackID, &(segment));
+  return 0;
+}
+
+void
+MediaEngineWebRTCVideoSource::GetName(nsAString& aName)
+{
+  char deviceName[KMaxDeviceNameLength];
+  memset(deviceName, 0, KMaxDeviceNameLength);
+
+  char uniqueId[KMaxUniqueIdLength];
+  memset(uniqueId, 0, KMaxUniqueIdLength);
+
+  if (mInitDone) {
+    mViECapture->GetCaptureDevice(
+      mCapIndex, deviceName, KMaxDeviceNameLength, uniqueId, KMaxUniqueIdLength
+    );
+    aName.Assign(NS_ConvertASCIItoUTF16(deviceName));
+  }
+}
+
+void
+MediaEngineWebRTCVideoSource::GetUUID(nsAString& aUUID)
+{
+  char deviceName[KMaxDeviceNameLength];
+  memset(deviceName, 0, KMaxDeviceNameLength);
+
+  char uniqueId[KMaxUniqueIdLength];
+  memset(uniqueId, 0, KMaxUniqueIdLength);
+
+  if (mInitDone) {
+    mViECapture->GetCaptureDevice(
+      mCapIndex, deviceName, KMaxDeviceNameLength, uniqueId, KMaxUniqueIdLength
+    );
+    aUUID.Assign(NS_ConvertASCIItoUTF16(uniqueId));
+  }
+}
+
+already_AddRefed<nsDOMMediaStream>
+MediaEngineWebRTCVideoSource::Allocate()
+{
+  if (mState != kReleased) {
+    return NULL;
+  }
+
+  char deviceName[KMaxDeviceNameLength];
+  memset(deviceName, 0, KMaxDeviceNameLength);
+
+  char uniqueId[KMaxUniqueIdLength];
+  memset(uniqueId, 0, KMaxUniqueIdLength);
+
+  mViECapture->GetCaptureDevice(
+    mCapIndex, deviceName, KMaxDeviceNameLength, uniqueId, KMaxUniqueIdLength
+  );
+
+  if (mViECapture->AllocateCaptureDevice(uniqueId, KMaxUniqueIdLength, mCapIndex) == 0) {
+    mState = kAllocated;
+    if (mViECapture->StartCapture(mCapIndex) == -1) {
+      return NULL;
+    }
+    return nsDOMMediaStream::CreateInputStream();
+  }
+
+  return NULL;
+}
+
+nsresult
+MediaEngineWebRTCVideoSource::Deallocate()
+{
+  if (mState != kStopped && mState != kAllocated) {
+    return NS_ERROR_FAILURE;
+  }
+
+  mViECapture->StopCapture(mCapIndex);
+  mViECapture->ReleaseCaptureDevice(mCapIndex);
+  mState = kReleased;
+  return NS_OK;
+}
+
+MediaEngineVideoOptions
+MediaEngineWebRTCVideoSource::GetOptions()
+{
+  MediaEngineVideoOptions aOpts;
+  aOpts.mWidth = mWidth;
+  aOpts.mHeight = mHeight;
+  aOpts.mMaxFPS = mFps;
+  aOpts.codecType = kVideoCodecI420;
+  return aOpts;
+}
+
+nsresult
+MediaEngineWebRTCVideoSource::Start(SourceMediaStream* aStream, TrackID aID)
+{
+  int error = 0;
+  if (!mInitDone || mState != kAllocated) {
+    return NS_ERROR_FAILURE;
+  }
+
+  if (!aStream) {
+    return NS_ERROR_FAILURE;
+  }
+
+  if (mState == kStarted) {
+    return NS_OK;
+  }
+
+  mSource = aStream;
+  mTrackID = aID;
+
+  // Setup  a blank track.
+  mImageContainer = layers::LayerManager::CreateImageContainer();
+  mSource->AddTrack(aID, mFps, 0, new VideoSegment());
+  mSource->AdvanceKnownTracksTime(STREAM_TIME_MAX);
+
+  error = mViERender->AddRenderer(mCapIndex, webrtc::kVideoI420, (webrtc::ExternalRenderer*)this);
+  if (error == -1) {
+    return NS_ERROR_FAILURE;
+  }
+
+  error = mViERender->StartRender(mCapIndex);
+  if (error == -1) {
+    return NS_ERROR_FAILURE;
+  }
+
+  mState = kStarted;
+  return NS_OK;
+}
+
+nsresult
+MediaEngineWebRTCVideoSource::Stop()
+{
+  if (mState != kStarted) {
+    return NS_ERROR_FAILURE;
+  }
+
+  mSource->EndTrack(mTrackID);
+  mSource->Finish();
+
+  mViERender->StopRender(mCapIndex);
+  mViERender->RemoveRenderer(mCapIndex);
+
+  mState = kStopped;
+  return NS_OK;
+}
+
+nsresult
+MediaEngineWebRTCVideoSource::Snapshot(PRUint32 aDuration, nsIDOMFile** aFile)
+{
+  /**
+   * To get a Snapshot we do the following:
+   * - Set a condition variable (mInSnapshotMode) to true
+   * - Attach the external renderer and start the camera
+   * - Wait for the condition variable to change to false
+   *
+   * Starting the camera has the effect of invoking DeliverFrame() when
+   * the first frame arrives from the camera. We only need one frame for
+   * GetCaptureDeviceSnapshot to work, so we immediately set the condition
+   * variable to false and notify this method.
+   *
+   * This causes the current thread to continue (PR_CondWaitVar will return),
+   * at which point we can grab a snapshot, convert it to a file and
+   * return from this function after cleaning up the temporary stream object
+   * and caling Stop() on the media source.
+   */
+  *aFile = nsnull;
+  if (!mInitDone || mState != kAllocated) {
+    return NS_ERROR_FAILURE;
+  }
+
+  mSnapshotLock = PR_NewLock();
+  mSnapshotCondVar = PR_NewCondVar(mSnapshotLock);
+
+  PR_Lock(mSnapshotLock);
+  mInSnapshotMode = true;
+  nsRefPtr<nsDOMMediaStream> stream = nsDOMMediaStream::CreateInputStream();
+
+  // Start the rendering (equivalent to calling Start(), but without a track).
+  int error = 0;
+  if (!mInitDone || mState != kAllocated) {
+    return NS_ERROR_FAILURE;
+  }
+  error = mViERender->AddRenderer(mCapIndex, webrtc::kVideoI420, (webrtc::ExternalRenderer*)this);
+  if (error == -1) {
+    return NS_ERROR_FAILURE;
+  }
+  error = mViERender->StartRender(mCapIndex);
+  if (error == -1) {
+    return NS_ERROR_FAILURE;
+  }
+
+  // Wait for the condition variable, will be set in DeliverFrame.
+  // We use a while loop, because even if PR_WaitCondVar returns, it's not
+  // guaranteed that the condition variable changed.
+  while (mInSnapshotMode) {
+    PR_WaitCondVar(mSnapshotCondVar, PR_INTERVAL_NO_TIMEOUT);
+  }
+
+  // If we get here, DeliverFrame received at least one frame.
+  PR_Unlock(mSnapshotLock);
+  PR_DestroyCondVar(mSnapshotCondVar);
+  PR_DestroyLock(mSnapshotLock);
+
+  webrtc::ViEFile* vieFile = webrtc::ViEFile::GetInterface(mVideoEngine);
+  if (!vieFile) {
+    return NS_ERROR_FAILURE;
+  }
+
+  // Create a temporary file and put the snapshot in it.
+  nsCOMPtr<nsIFile> tmp;
+  nsresult rv = NS_GetSpecialDirectory("TmpD", getter_AddRefs(tmp));
+  NS_ENSURE_SUCCESS(rv, rv);
+
+  tmp->Append(NS_LITERAL_STRING("webrtc_snapshot.jpeg"));
+  rv = tmp->CreateUnique(nsIFile::NORMAL_FILE_TYPE, 0600);
+  NS_ENSURE_SUCCESS(rv, rv);
+
+  nsString unicodePath;
+  rv = tmp->GetPath(unicodePath);
+  NS_ENSURE_SUCCESS(rv, rv);
+
+  const char* path = NS_ConvertUTF16toUTF8(unicodePath).get();
+  if (vieFile->GetCaptureDeviceSnapshot(mCapIndex, path) < 0) {
+    return NS_ERROR_FAILURE;
+  }
+
+  // Stop the camera.
+  mViERender->StopRender(mCapIndex);
+  mViERender->RemoveRenderer(mCapIndex);
+
+  NS_ADDREF(*aFile = new nsDOMFileFile(tmp));
+
+  return NS_OK;
+}
+
+/**
+ * Initialization and Shutdown functions for the video source, called by the
+ * constructor and destructor respectively.
+ */
+
+void
+MediaEngineWebRTCVideoSource::Init()
+{
+  if (mVideoEngine == NULL) {
+    return;
+  }
+
+  mViEBase = webrtc::ViEBase::GetInterface(mVideoEngine);
+  if (mViEBase == NULL) {
+    return;
+  }
+
+  // Get interfaces for capture, render for now
+  mViECapture = webrtc::ViECapture::GetInterface(mVideoEngine);
+  mViERender = webrtc::ViERender::GetInterface(mVideoEngine);
+
+  if (mViECapture == NULL || mViERender == NULL) {
+    return;
+  }
+
+  // Temporary logging
+  mVideoEngine->SetTraceFilter(webrtc::kTraceAll);
+  mVideoEngine->SetTraceFile("Vievideotrace.out");
+
+  mInitDone = true;
+}
+
+void
+MediaEngineWebRTCVideoSource::Shutdown()
+{
+  int error = 0;
+  bool continueShutdown = false;
+
+  if (!mInitDone) {
+    return;
+  }
+
+  if (mState == kStarted) {
+    error = mViERender->StopRender(mCapIndex);
+    error = mViERender->RemoveRenderer(mCapIndex);
+    continueShutdown = true;
+  }
+
+  error = 0;
+  if (mState == kAllocated || continueShutdown) {
+    error = mViECapture->StopCapture(mCapIndex);
+    error = mViECapture->ReleaseCaptureDevice(mCapIndex);
+    continueShutdown = false;
+  }
+
+  mViECapture->Release();
+  mViERender->Release();
+  mViEBase->Release();
+  mState = kReleased;
+  mInitDone = false;
+}
+
+
+
+/**
+ * Webrtc audio source.
+ */
+
+NS_IMPL_THREADSAFE_ISUPPORTS1(MediaEngineWebRTCAudioSource, nsITimerCallback)
+
+//static initialization
+const unsigned int MediaEngineWebRTCAudioSource::PLAYOUT_SAMPLE_FREQUENCY = 16000;
+const unsigned int MediaEngineWebRTCAudioSource::PLAYOUT_SAMPLE_LENGTH  = 160;
+const unsigned int MediaEngineWebRTCAudioSource::KMaxDeviceNameLength = 128;
+const unsigned int MediaEngineWebRTCAudioSource::KMaxUniqueIdLength = 128;
+
+// Performs very basic & common initialization
+void
+MediaEngineWebRTCAudioSource::Init()
+{
+  if(NULL == mVoiceEngine)
+   return;
+
+  mVoEBase = webrtc::VoEBase::GetInterface(mVoiceEngine);
+  if (NULL == mVoEBase )
+  {
+    printf( "ERROR in AudioEngine::VoEBase ");
+    return;
+  }
+
+  //get interfaces for capture, render for now
+  mVoEHw = webrtc::VoEHardware::GetInterface(mVoiceEngine);
+
+  // check if all the interfaces were ok till now
+  if(NULL == mVoEHw)
+   return ;
+
+  mChannel = mVoEBase->CreateChannel();
+  if(-1 == mChannel)
+  {
+ return;
+  }
+
+  mVoEXmedia = webrtc::VoEExternalMedia::GetInterface(mVoiceEngine);
+  if(NULL == mVoEXmedia)
+  {
+    return;
+  }
+  //temp logging
+  mVoiceEngine->SetTraceFilter(webrtc::kTraceAll);
+  mVoiceEngine->SetTraceFile( "Voevideotrace.out" );
+  mAudioSegment.Init(CHANNELS);
+
+  // we should be good by now
+  mInitDone = true;
+
+}
+
+void
+MediaEngineWebRTCAudioSource::GetName(nsAString& aName)
+{
+  char deviceName[KMaxDeviceNameLength];
+  memset(deviceName, 0, KMaxDeviceNameLength);
+  char uniqueId[KMaxUniqueIdLength];
+  memset(uniqueId, 0, KMaxUniqueIdLength);
+  if(true == mInitDone)
+  {
+    mVoEHw->GetRecordingDeviceName(
+             mCapIndex, deviceName,  uniqueId );
+   aName.Assign(NS_ConvertASCIItoUTF16(deviceName));
+  }
+
+  return;
+}
+
+void
+MediaEngineWebRTCAudioSource::GetUUID(nsAString& aUUID)
+{
+  char deviceName[KMaxDeviceNameLength];
+  memset(deviceName, 0, KMaxDeviceNameLength);
+  char uniqueId[KMaxUniqueIdLength];
+  memset(uniqueId, 0, KMaxUniqueIdLength);
+  if(true == mInitDone )
+  {
+    mVoEHw->GetRecordingDeviceName(
+             mCapIndex, deviceName,  uniqueId );
+   aUUID.Assign(NS_ConvertASCIItoUTF16(uniqueId));
+  }
+
+  return;
+}
+
+already_AddRefed<nsDOMMediaStream>
+MediaEngineWebRTCAudioSource::Allocate()
+{
+  if (mState != kReleased) {
+    return NULL;
+  }
+  // no special webrtc code to be done , i hope
+  mState = kAllocated;
+  return nsDOMMediaStream::CreateInputStream();
+}
+
+nsresult
+MediaEngineWebRTCAudioSource::Deallocate()
+{
+  if (mState != kStopped && mState != kAllocated) {
+    return NS_ERROR_FAILURE;
+  }
+  mState = kReleased;
+  return NS_OK;
+}
+
+//Loop back audio through media-stream
+nsresult
+MediaEngineWebRTCAudioSource::Start(SourceMediaStream* aStream, TrackID aID)
+{
+  const int DEFAULT_PORT = 55555;
+  printf("\n MediaEngineWebRTCAudioSource : Start: Entered ");
+  if (false == mInitDone || mState != kAllocated) {
+    return NS_ERROR_FAILURE;
+  }
+
+  if(!aStream)
+   return NS_ERROR_FAILURE;
+
+  mTimer = do_CreateInstance(NS_TIMER_CONTRACTID);
+  if (!mTimer) {
+    return NS_ERROR_FAILURE;
+  }
+
+  mSource = aStream;
+
+  AudioSegment* segment = new AudioSegment();
+  segment->Init(CHANNELS);
+  //segment->InsertNullDataAtStart(1);
+  mSource->AddTrack(aID, PLAYOUT_SAMPLE_FREQUENCY, 0, segment);
+  mSource->AdvanceKnownTracksTime(STREAM_TIME_MAX);
+  mTrackID = aID;
+
+  printf("\n Starting the audio engine ");
+  mVoEBase->SetLocalReceiver(mChannel,DEFAULT_PORT);
+  mVoEBase->SetSendDestination(mChannel,DEFAULT_PORT,"127.0.0.1");
+
+  if(-1 == mVoEXmedia->SetExternalPlayoutStatus(true)) {
+    printf("\n SetExternalPlayoutStatus failed %d ", mVoEBase->LastError() );
+   return NS_ERROR_FAILURE;
+  }
+  //loopback audio
+  mVoEBase->StartPlayout(mChannel);
+  mVoEBase->StartReceive(mChannel);
+  mVoEBase->StartSend(mChannel);
+
+  mState = kStarted;
+  // call every 10 milliseconds
+  mTimer->InitWithCallback(this, 10, nsITimer::TYPE_REPEATING_SLACK);
+  return NS_OK;
+}
+
+nsresult
+MediaEngineWebRTCAudioSource::Stop()
+{
+  if (mState != kStarted) {
+    return NS_ERROR_FAILURE;
+  }
+  if (!mTimer) {
+    return NS_ERROR_FAILURE;
+  }
+
+  if(!mVoEBase) {
+    return NS_ERROR_FAILURE;
+  }
+
+  mVoEBase->StopReceive(mChannel);
+  mVoEBase->StopSend(mChannel);
+  mVoEBase->StopPlayout(mChannel);
+
+  mTimer->Cancel();
+  mTimer = NULL;
+  mState = kStopped;
+  return NS_OK;
+}
+
+nsresult
+MediaEngineWebRTCAudioSource::Snapshot(PRUint32 aDuration, nsIDOMFile** aFile)
+{
+   return NS_ERROR_NOT_IMPLEMENTED;
+}
+
+
+void
+MediaEngineWebRTCAudioSource::Shutdown()
+{
+  if(false == mInitDone)
+   return;
+
+  if(kStarted == mState)
+  {
+   //Stop External playout
+   mVoEBase->StopReceive(mChannel);
+   mVoEBase->StopSend(mChannel);
+   mVoEBase->StopPlayout(mChannel);
+   mTimer->Cancel();
+   mTimer = NULL;
+  }
+
+  mVoEBase->Terminate();
+  mVoEXmedia->Release();
+  mVoEHw->Release();
+  mVoEBase->Release();
+  mState = kReleased;
+  mInitDone = false;
+}
+
+
+NS_IMETHODIMP
+MediaEngineWebRTCAudioSource::Notify(nsITimer* aTimer)
+{
+  // just one audio sample
+  static int16_t audio10ms[PLAYOUT_SAMPLE_LENGTH];
+  int sample_length  =0;
+  memset(audio10ms, 0, PLAYOUT_SAMPLE_LENGTH * sizeof(short));
+  mVoEXmedia->ExternalPlayoutGetData(audio10ms, PLAYOUT_SAMPLE_FREQUENCY, 100, sample_length); 
+  if(sample_length == 0)
+  {
+ return NS_OK;
+  }
+
+  // allocate shared buffer of lenght bytes
+  int buff_size = PLAYOUT_SAMPLE_LENGTH * sizeof(short);
+  nsRefPtr<SharedBuffer> buffer = SharedBuffer::Create(buff_size);
+  //buffer->AddRef();
+  int16_t* dest = static_cast<int16_t*>(buffer->Data());
+  for(int i=0; i < sample_length; i++)
+  {
+ dest[i] = audio10ms[i];
+  }
+
+  mAudioSegment.AppendFrames(buffer.forget(), sample_length, 0, sample_length, nsAudioStream::FORMAT_S16_LE);
+  mSource->AppendToTrack(mTrackID, &mAudioSegment);
+  return NS_OK;
+}
+
+//XXX: Not multi-threded and non-renterant -???
+void
+MediaEngineWebRTC::EnumerateVideoDevices(nsTArray<nsRefPtr<MediaEngineVideoSource> >* aVSources)
+{
+  int error = 0;
+  printf("\n EnumerateVideo Devices ");
+
+  //we can use it function locals .. no need to save it in the class
+  webrtc::ViEBase* ptrViEBase;
+  webrtc::ViECapture* ptrViECapture;
+
+  if(NULL == mVideoEngine)
+  {
+      mVideoEngine = webrtc::VideoEngine::Create();
+      if (NULL == mVideoEngine )
+ {
+         printf( "ERROR in VideoEngine::Create\n");
+        return;
+     }
+  }
+
+  // te should have VideoEngine Created here
+  ptrViEBase = webrtc::ViEBase::GetInterface(mVideoEngine);
+  if (NULL == ptrViEBase) {
+    return;
+  }
+
+
+  if( false == mVideoEngineInit)
+  {
+   error = ptrViEBase->Init();
+   if (-1 == error ) {
+      printf( "ERROR in VideoEngine::Init\n");
+      return;
+   }
+    mVideoEngineInit = true;
+  }
+
+  ptrViECapture = webrtc::ViECapture::GetInterface(mVideoEngine);
+  if (NULL == ptrViECapture ) {
+    printf("ERROR in ViECapture::GetInterface\n");
+    return;
+  }
+
+  int num = ptrViECapture->NumberOfCaptureDevices();
+  if (num <= 0) {
+    printf( "ERROR no video devices found\n");
+    return;
+  } else {
+    printf("GetUserMedia:: Found %d devices!\n", num);
+  }
+
+  for(int i=0; i < num; i++)
+  {
+    //let's create VideoSouce objects
+    nsRefPtr<MediaEngineVideoSource> vSource = new MediaEngineWebRTCVideoSource(mVideoEngine , i);
+    aVSources->AppendElement( vSource );
+  }
+
+  // safe to release local interfaces on the engine
+   ptrViEBase->Release();
+   ptrViECapture->Release();
+
+  return;
+}
+
+void
+MediaEngineWebRTC::EnumerateAudioDevices(nsTArray<nsRefPtr<MediaEngineAudioSource> >* aASources)
+{
+  // Be sure to release these at the end of the function
+  // safety measure though
+  webrtc::VoEBase* ptrVoEBase = NULL;
+  webrtc::VoEHardware* ptrVoEHw = NULL;
+  int error = 0;
+
+  if(NULL == mVoiceEngine)
+  {
+    mVoiceEngine = webrtc::VoiceEngine::Create();
+    if(NULL == mVoiceEngine)
+    {
+ printf(" Unable to create voice engine ");
+ return;
+    }
+  }
+
+  // all interfaces pointers are ref-counted
+  // we should be good on mutiple calls to this function ..
+  ptrVoEBase = webrtc::VoEBase::GetInterface( mVoiceEngine );
+  if(NULL == ptrVoEBase)
+  {
+ printf(" VoEBase creation failed ");
+ return;
+  }
+
+  //Init the voice library for the common parts
+  if(false == mAudioEngineInit)
+  {
+ error = ptrVoEBase->Init();
+ if(-1 == error)
+ {
+   printf("\n Audio Engine Init Failed ");
+ }
+ mAudioEngineInit = true;
+  }
+
+  ptrVoEHw = webrtc::VoEHardware::GetInterface(mVoiceEngine);
+  if(NULL == ptrVoEHw)
+  {
+ printf("\n Unable to get Audio Hardware Interface ");
+    return;
+  }
+
+  //let's enumerate audio input devices
+  int nDevices = 0;
+  error = ptrVoEHw->GetNumOfRecordingDevices(nDevices);
+  printf("\n Number if devices is %d ",nDevices);
+  for(int i = 0; i < nDevices; i++)
+  {
+     char deviceName[128];
+   memset(deviceName, 0, 128);
+   char uniqueId[128];
+   memset(uniqueId, 0, 128);
+     ptrVoEHw->GetRecordingDeviceName(i, deviceName,  uniqueId );
+        nsRefPtr<MediaEngineAudioSource> aSource = new MediaEngineWebRTCAudioSource(mVoiceEngine , i);
+ aSource->AddRef();
+   aASources->AppendElement( aSource );
+  }
+
+  // releasing them to re-set the reference count ...
+  ptrVoEHw->Release();
+  ptrVoEBase->Release();
+}
+
+
+void
+MediaEngineWebRTC::Shutdown()
+{
+
+  if(mVideoEngine)
+    webrtc::VideoEngine::Delete(mVideoEngine);
+
+  if(mVoiceEngine)
+   webrtc::VoiceEngine::Delete(mVoiceEngine);
+
+  mVideoEngine = NULL;
+  mVoiceEngine = NULL;
+}
+
+}
diff --git a/dom/media/Makefile.in b/dom/media/Makefile.in
--- a/dom/media/Makefile.in
+++ b/dom/media/Makefile.in
@@ -34,5 +34,9 @@
   MediaManager.cpp \
   $(NULL)
 
+LOCAL_INCLUDES += \
+	-I$(topsrcdir)/media/webrtc/trunk/src \
+	$(NULL)
+
 include $(topsrcdir)/config/config.mk
 include $(topsrcdir)/config/rules.mk
diff --git a/dom/media/MediaManager.cpp b/dom/media/MediaManager.cpp
--- a/dom/media/MediaManager.cpp
+++ b/dom/media/MediaManager.cpp
@@ -5,8 +5,6 @@
 #include "MediaManager.h"
 
 #include "MediaStreamGraph.h"
-#include "MediaEngineDefault.h"
-
 #include "nsIDOMFile.h"
 #include "nsIEventTarget.h"
 #include "nsIScriptGlobalObject.h"
@@ -16,6 +14,13 @@
 #include "nsDOMFile.h"
 #include "nsGlobalWindow.h"
 
+/* Using WebRTC backend on Desktops (Mac, Windows, Linux), otherwise default */
+#if defined(XP_WIN) || defined(XP_UNIX)
+#include "MediaEngineWebRTC.h"
+#else
+#include "MediaEngineDefault.h"
+#endif
+
 namespace mozilla {
 
 /**
@@ -64,7 +69,7 @@
     , mWindowID(aWindowID) {}
 
   SuccessCallbackRunnable(nsIDOMGetUserMediaSuccessCallback* aSuccess,
-    nsIDOMMediaStream* aStream, PRUint64 aWindowID)
+    already_AddRefed<nsDOMMediaStream> aStream, PRUint64 aWindowID)
     : mSuccess(aSuccess)
     , mStream(aStream)
     , mWindowID(aWindowID) {}
@@ -93,155 +98,13 @@
 };
 
 /**
- * This runnable creates a nsDOMMediaStream from a given MediaEngineSource
- * and returns it via a success callback. Both must be done on the main thread.
- */
-class GetUserMediaCallbackRunnable : public nsRunnable
-{
-public:
-  GetUserMediaCallbackRunnable(MediaEngineSource* aSource, TrackID aId,
-    nsIDOMGetUserMediaSuccessCallback* aSuccess,
-    nsIDOMGetUserMediaErrorCallback* aError,
-    PRUint64 aWindowID,
-    StreamListeners* aListeners)
-    : mSource(aSource)
-    , mId(aId)
-    , mSuccess(aSuccess)
-    , mError(aError)
-    , mWindowID(aWindowID)
-    , mListeners(aListeners) {}
-
-  NS_IMETHOD
-  Run()
-  {
-    /**
-     * Normally we would now get the name & UUID for the device and ask the
-     * user permission. We will do that when we have some UI. Currently,
-     * only the Android {picture:true} backend is functional, which does not
-     * need a permission prompt, as permission is implicit by user action.
-     *
-     * See bug 748835 for progress on the desktop UI.
-     */
-    nsCOMPtr<nsDOMMediaStream> comStream = mSource->Allocate();
-    if (!comStream) {
-      NS_DispatchToMainThread(new ErrorCallbackRunnable(
-        mError, NS_LITERAL_STRING("HARDWARE_UNAVAILABLE"), mWindowID
-      ));
-      return NS_OK;
-    }
-
-    // Add our listener. We'll call Start() on the source when get a callback
-    // that the MediaStream has started consuming. The listener is freed
-    // when the page is invalidated (on navigation or close).
-    GetUserMediaCallbackMediaStreamListener* listener =
-      new GetUserMediaCallbackMediaStreamListener(mSource, comStream, mId);
-    comStream->GetStream()->AddListener(listener);
-
-    {
-      MutexAutoLock lock(*(MediaManager::Get()->GetLock()));
-      mListeners->AppendElement(listener);
-    }
-
-    // Add the listener to CallbackRunnables so it can be invalidated.
-    NS_DispatchToMainThread(new SuccessCallbackRunnable(
-      mSuccess, comStream.get(), mWindowID
-    ));
-    return NS_OK;
-  }
-
-private:
-  nsCOMPtr<MediaEngineSource> mSource;
-  TrackID mId;
-  nsCOMPtr<nsIDOMGetUserMediaSuccessCallback> mSuccess;
-  nsCOMPtr<nsIDOMGetUserMediaErrorCallback> mError;
-  PRUint64 mWindowID;
-  StreamListeners* mListeners;
-};
-
-/**
- * This runnable creates a nsIDOMFile from a MediaEngineVideoSource and
- * passes the result back via a SuccessRunnable. Both must be done on the
- * main thread.
- */
-class GetUserMediaSnapshotCallbackRunable : public nsRunnable
-{
-public:
-  GetUserMediaSnapshotCallbackRunable(MediaEngineSource* aSource,
-    PRUint32 aDuration,
-    nsIDOMGetUserMediaSuccessCallback* aSuccessCallback,
-    nsIDOMGetUserMediaErrorCallback* aErrorCallback,
-    nsPIDOMWindow* aWindow)
-    : mSource(aSource)
-    , mDuration(aDuration)
-    , mSuccessCallback(aSuccessCallback)
-    , mErrorCallback(aErrorCallback)
-    , mWindow(aWindow) {}
-
-  NS_IMETHOD
-  Run()
-  {
-    mWindowID = mWindow->WindowID();
-
-    // Before getting a snapshot, check if page is allowed to open a popup.
-    // We do this because {picture:true} on all platforms will open a new
-    // "window" to let the user preview or select an image.
-
-    if (mWindow->GetPopupControlState() <= openControlled) {
-      return NS_OK;
-    }
-    
-    nsCOMPtr<nsIPopupWindowManager> pm =
-      do_GetService(NS_POPUPWINDOWMANAGER_CONTRACTID);
-    if (!pm) {
-      return NS_OK;
-    }
-
-    PRUint32 permission;
-    nsCOMPtr<nsIDocument> doc = mWindow->GetExtantDoc();
-    pm->TestPermission(doc->GetDocumentURI(), &permission);
-    if (permission == nsIPopupWindowManager::DENY_POPUP) {
-      nsCOMPtr<nsIDOMDocument> domDoc = mWindow->GetExtantDocument();
-      nsGlobalWindow::FirePopupBlockedEvent(
-        domDoc, mWindow, nsnull, EmptyString(), EmptyString()
-      );
-      return NS_OK;
-    }
-
-    nsCOMPtr<nsDOMMediaStream> comStream = mSource->Allocate();
-    if (!comStream) {
-      NS_DispatchToMainThread(new ErrorCallbackRunnable(
-        mErrorCallback, NS_LITERAL_STRING("HARDWARE_UNAVAILABLE"), mWindowID
-      ));
-      return NS_OK;
-    }
-
-    nsCOMPtr<nsIDOMFile> file;
-    mSource->Snapshot(mDuration, getter_AddRefs(file));
-    mSource->Deallocate();
-
-    NS_DispatchToMainThread(new SuccessCallbackRunnable(
-      mSuccessCallback, file, mWindowID
-    ));
-    return NS_OK;
-  }
-
-private:
-  nsCOMPtr<MediaEngineSource> mSource;
-  PRUint32 mDuration;
-  nsCOMPtr<nsIDOMGetUserMediaSuccessCallback> mSuccessCallback;
-  nsCOMPtr<nsIDOMGetUserMediaErrorCallback>  mErrorCallback;
-  nsCOMPtr<nsPIDOMWindow> mWindow;
-
-  PRUint64 mWindowID;
-};
-
-/**
  * Runs on a seperate thread and is responsible for enumerating devices.
  * Depending on whether a picture or stream was asked for, either
- * GetUserMediaCallbackRunnable or GetUserMediaSnapshotCallbackRunnable
- * will be dispatched to the main thread to return the result to DOM.
+ * ProcessGetUserMedia or ProcessGetUserMediaSnapshot is called, and the results
+ * are sent back to the DOM.
  *
- * Do not run this on the main thread.
+ * Do not run this on the main thread. The success and error callbacks *MUST*
+ * be dispatched on the main thread!
  */
 class GetUserMediaRunnable : public nsRunnable
 {
@@ -298,6 +161,101 @@
     return NS_OK;
   }
 
+  /**
+   * Allocates a video or audio device and returns a MediaStream via
+   * a SuccessRunnable or an error via the ErrorRunnable. Off the main thread.
+   */
+  void
+  ProcessGetUserMedia(MediaEngineSource* aSource, TrackID aId)
+  {
+    /**
+     * Normally we would now get the name & UUID for the device and ask the
+     * user permission. We will do that when we have some UI. Currently,
+     * only the Android {picture:true} backend is functional, which does not
+     * need a permission prompt, as permission is implicit by user action.
+     *
+     * See bug 748835 for progress on the desktop UI.
+     */
+    nsCOMPtr<nsDOMMediaStream> comStream = aSource->Allocate();
+    if (!comStream) {
+      NS_DispatchToMainThread(new ErrorCallbackRunnable(
+        mError, NS_LITERAL_STRING("HARDWARE_UNAVAILABLE"), mWindowID
+      ));
+      return;
+    }
+
+    // Add our listener. We'll call Start() on the source when get a callback
+    // that the MediaStream has started consuming. The listener is freed
+    // when the page is invalidated (on navigation or close).
+    GetUserMediaCallbackMediaStreamListener* listener =
+      new GetUserMediaCallbackMediaStreamListener(aSource, comStream, aId);
+    comStream->GetStream()->AddListener(listener);
+
+    {
+      MutexAutoLock lock(*(MediaManager::Get()->GetLock()));
+      mListeners->AppendElement(listener);
+    }
+
+    // Add the listener to CallbackRunnables so it can be invalidated.
+    NS_DispatchToMainThread(new SuccessCallbackRunnable(
+      mSuccess, comStream.forget(), mWindowID
+    ));
+    return;
+  }
+
+  /**
+   * Allocates a video device, takes a snapshot and returns a DOMFile via
+   * a SuccessRunnable or an error via the ErrorRunnable. Off the main thread.
+   */
+  void
+  ProcessGetUserMediaSnapshot(MediaEngineSource* aSource, int aDuration)
+  {
+    // Before getting a snapshot, check if page is allowed to open a popup.
+    // We do this because {picture:true} on all platforms will open a new
+    // "window" to let the user preview or select an image.
+
+    if (mWindow->GetPopupControlState() <= openControlled) {
+      return;
+    }
+
+    // On Desktops, no popup is displayed, only a doorhanger.
+#if !defined(XP_WIN) && !defined(XP_UNIX)
+    nsCOMPtr<nsIPopupWindowManager> pm =
+      do_GetService(NS_POPUPWINDOWMANAGER_CONTRACTID);
+    if (!pm) {
+      return;
+    }
+
+    PRUint32 permission;
+    nsCOMPtr<nsIDocument> doc = mWindow->GetExtantDoc();
+    pm->TestPermission(doc->GetDocumentURI(), &permission);
+    if (permission == nsIPopupWindowManager::DENY_POPUP) {
+      nsCOMPtr<nsIDOMDocument> domDoc = mWindow->GetExtantDocument();
+      nsGlobalWindow::FirePopupBlockedEvent(
+        domDoc, mWindow, nsnull, EmptyString(), EmptyString()
+      );
+      return;
+    }
+#endif
+
+    nsCOMPtr<nsDOMMediaStream> comStream = aSource->Allocate();
+    if (!comStream) {
+      NS_DispatchToMainThread(new ErrorCallbackRunnable(
+        mError, NS_LITERAL_STRING("HARDWARE_UNAVAILABLE"), mWindowID
+      ));
+      return;
+    }
+
+    nsCOMPtr<nsIDOMFile> file;
+    aSource->Snapshot(aDuration, getter_AddRefs(file));
+    aSource->Deallocate();
+
+    NS_DispatchToMainThread(new SuccessCallbackRunnable(
+      mSuccess, file, mWindowID
+    ));
+    return;
+  }
+
   // {picture:true}
   void
   SendPicture()
@@ -312,10 +270,9 @@
       ));
       return;
     }
+
     MediaEngineVideoSource* videoSource = videoSources[count - 1];
-    NS_DispatchToMainThread(new GetUserMediaSnapshotCallbackRunable(
-      videoSource, 0 /* duration */, mSuccess, mError, mWindow
-    ));
+    ProcessGetUserMediaSnapshot(videoSource, 0 /* duration */);
   }
 
   // {video:true}
@@ -334,9 +291,7 @@
     }
 
     MediaEngineVideoSource* videoSource = videoSources[count - 1];
-    NS_DispatchToMainThread(new GetUserMediaCallbackRunnable(
-      videoSource, kVideoTrack, mSuccess, mError, mWindowID, mListeners
-    ));
+    ProcessGetUserMedia(videoSource, kVideoTrack);
   }
 
   // {audio:true}
@@ -355,9 +310,7 @@
     }
 
     MediaEngineAudioSource* audioSource = audioSources[count - 1];
-    NS_DispatchToMainThread(new GetUserMediaCallbackRunnable(
-      audioSource, kAudioTrack, mSuccess, mError, mWindowID, mListeners
-    ));
+    ProcessGetUserMedia(audioSource, kAudioTrack);
   }
 
 private:
@@ -415,7 +368,7 @@
     mActiveWindows.Put(windowID, listeners);
   }
 
-  // Pass runanbles along to GetUserMediaRunnable so it can add the
+  // Pass runnables along to GetUserMediaRunnable so it can add the
   // MediaStreamListener to the runnable list.
   nsCOMPtr<nsIRunnable> gUMRunnable = new GetUserMediaRunnable(
     audio, video, picture, onSuccess, onError, aWindow, listeners
@@ -434,11 +387,16 @@
 MediaEngine*
 MediaManager::GetBackend()
 {
-  // Plugin backends as appropriate. Only default is available for now, which
-  // also includes picture support for Android.
+  // Plugin backends as appropriate. The default engine also currently
+  // includes picture support for Android.
   if (!mBackend) {
+#if defined(XP_WIN) || defined(XP_UNIX)
+    mBackend = new MediaEngineWebRTC();
+#else
     mBackend = new MediaEngineDefault();
+#endif
   }
+
   return mBackend;
 }
 
diff --git a/dom/media/MediaManager.h b/dom/media/MediaManager.h
--- a/dom/media/MediaManager.h
+++ b/dom/media/MediaManager.h
@@ -16,6 +16,32 @@
 namespace mozilla {
 
 /**
+ * This runnable is needed to start the media source on the main thread.
+ * It is used by the StreamListener in the NotifyConsumptionChanged callback.
+ */
+class StartMediaRunnable : public nsRunnable
+{
+public:
+  StartMediaRunnable(MediaEngineSource* aSource, SourceMediaStream* aStream,
+    TrackID aListenId)
+  : mSource(aSource)
+  , mStream(aStream)
+  , mId(aListenId) {}
+
+  NS_IMETHOD
+  Run()
+  {
+    mSource->Start(mStream, mId);
+    return NS_OK;
+  }
+
+private:
+  nsRefPtr<MediaEngineSource> mSource;
+  SourceMediaStream* mStream;
+  TrackID mId;
+};
+
+/**
  * This class is an implementation of MediaStreamListener. This is used
  * to Start() and Stop() the underlying MediaEngineSource when MediaStreams
  * are assigned and deassigned in content.
@@ -47,7 +73,9 @@
   {
     if (aConsuming == CONSUMED) {
       nsRefPtr<SourceMediaStream> stream = mStream->GetStream()->AsSourceStream();
-      mSource->Start(stream, mId);
+      NS_DispatchToMainThread(new StartMediaRunnable(
+        mSource, stream, mId
+      ));
       return;
     }
 
@@ -65,7 +93,7 @@
   nsresult Run() { return NS_OK; }
 
 private:
-  nsCOMPtr<MediaEngineSource> mSource;
+  nsRefPtr<MediaEngineSource> mSource;
   nsCOMPtr<nsDOMMediaStream> mStream;
   TrackID mId;
   bool mValid;
diff --git a/layout/build/Makefile.in b/layout/build/Makefile.in
--- a/layout/build/Makefile.in
+++ b/layout/build/Makefile.in
@@ -129,6 +129,7 @@
 ifdef MOZ_MEDIA
 SHARED_LIBRARY_LIBS 	+= \
 	$(DEPTH)/content/media/$(LIB_PREFIX)gkconmedia_s.$(LIB_SUFFIX) \
+	$(DEPTH)/content/media/webrtc/$(LIB_PREFIX)gkconwebrtc_s.$(LIB_SUFFIX) \
 	$(NULL)
 endif
 
